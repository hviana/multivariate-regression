# ğŸŒŠ ESNRegression - Echo State Network for Multivariate Time Series

<div align="center">

**A powerful, deterministic Echo State Network implementation for online multivariate time series regression and forecasting**

[ğŸ“¦ Installation](#-installation) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“– Documentation](#-documentation) â€¢ [âš™ï¸ Configuration](#ï¸-configuration)

</div>

---

## ğŸ“‹ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸ“¦ Installation](#-installation)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ§  Core Concepts](#-core-concepts)
  - [Echo State Networks](#echo-state-networks-esn)
  - [Reservoir Computing](#reservoir-computing)
  - [Online Learning with RLS](#online-learning-with-rls)
  - [Multi-Horizon Forecasting](#multi-horizon-forecasting)
- [âš™ï¸ Configuration](#ï¸-configuration)
  - [Reservoir Parameters](#1-reservoir-parameters)
  - [Training Parameters](#2-training-parameters)
  - [Normalization Parameters](#3-normalization-parameters)
  - [Prediction Parameters](#4-prediction-parameters)
- [ğŸ“Š API Reference](#-api-reference)
- [ğŸ¯ Use Cases & Optimization](#-use-cases--optimization)
- [ğŸ’¾ Model Persistence](#-model-persistence)
- [ğŸ”¬ Advanced Topics](#-advanced-topics)
- [ğŸ“ License](#-license)

---

## âœ¨ Features

<div align="center">

| Feature | Description |
|---------|-------------|
| ğŸ”„ **Online Learning** | Incremental training without storing entire dataset |
| ğŸ¯ **Multi-Step Forecasting** | Predict multiple future time steps simultaneously |
| ğŸ”’ **Deterministic** | Same seed produces identical results every time |
| âš¡ **Zero-Allocation Hot Paths** | Efficient memory management for fit/predict |
| ğŸ“ˆ **Uncertainty Quantification** | Built-in confidence intervals for predictions |
| ğŸ›¡ï¸ **Outlier Robustness** | Automatic outlier detection and downweighting |
| ğŸ’¾ **Serializable** | Save and load trained models easily |
| ğŸ›ï¸ **Highly Configurable** | 25+ tunable parameters for optimization |

</div>

---

## ğŸ“¦ Installation

```typescript
import { ESNRegression } from "jsr:@hviana/multivariate-regression";
```

---

## ğŸš€ Quick Start

### Basic Example

```typescript
import { ESNRegression } from "jsr:@hviana/multivariate-regression";

// Create model instance
const esn = new ESNRegression({
  reservoirSize: 256,
  maxFutureSteps: 3,
  spectralRadius: 0.9,
});

// Prepare training data
// X: input features, Y: target values
const xTrain = [
  [1.0, 2.0],
  [1.5, 2.5],
  [2.0, 3.0],
  [2.5, 3.5],
  [3.0, 4.0],
];

// For direct multi-horizon with maxFutureSteps=3 and 1 target:
// Y should have shape [nSamples][nTargets * maxFutureSteps]
const yTrain = [
  [10, 11, 12],  // predictions for t+1, t+2, t+3
  [15, 16, 17],
  [20, 21, 22],
  [25, 26, 27],
  [30, 31, 32],
];

// Train online
const result = esn.fitOnline({ 
  xCoordinates: xTrain, 
  yCoordinates: yTrain 
});

console.log(`ğŸ“Š Samples processed: ${result.samplesProcessed}`);
console.log(`ğŸ“‰ Average loss: ${result.averageLoss.toFixed(6)}`);

// Predict future steps
const prediction = esn.predict(3);

console.log("\nğŸ”® Predictions:");
prediction.predictions.forEach((step, i) => {
  console.log(`  Step ${i + 1}: ${step.map(v => v.toFixed(2)).join(", ")}`);
});
```

---

## ğŸ§  Core Concepts

### Echo State Networks (ESN)

Echo State Networks are a type of Recurrent Neural Network (RNN) belonging to the **Reservoir Computing** paradigm. The key insight is that only the output layer needs training, while the recurrent connections remain fixed.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ECHO STATE NETWORK ARCHITECTURE              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚    â”‚          â”‚      â”‚                    â”‚      â”‚          â”‚   â”‚
â”‚    â”‚  INPU    â”‚â”€â”€â”€â”€â–¶â”‚     RESERVOIR      â”‚â”€â”€â”€â”€â–¶â”‚  OUTPUT  â”‚   â”‚
â”‚    â”‚  (X)     â”‚ Win  â”‚   (Fixed Weights)  â”‚ Wout â”‚   (Y)    â”‚   â”‚
â”‚    â”‚          â”‚      â”‚                    â”‚      â”‚          â”‚   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚     â–²                            â”‚
â”‚                              â”‚     â”‚                            â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                           W (recurrent)                         â”‚
â”‚                                                                 â”‚
â”‚    âœ… Win, W, bias: Randomly initialized, FIXED                 â”‚
â”‚    ğŸ¯ Wout: TRAINED via Recursive Least Squares (RLS)           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Advantages:**
- ğŸš€ **Fast training**: Only output weights are learned
- ğŸ”„ **Online learning**: Natural fit for streaming data
- ğŸ’¾ **Memory efficient**: No backpropagation through time
- ğŸ“Š **Stable dynamics**: Controlled via spectral radius

---

### Reservoir Computing

The reservoir acts as a high-dimensional nonlinear expansion of the input signal. Think of it as a "liquid" that transforms inputs into rich temporal features.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RESERVOIR DYNAMICS                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   State Update (Leaky Integrator):                              â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â”‚   r(t) = (1-Î±)Â·r(t-1) + Î±Â·act(WinÂ·x(t) + WÂ·r(t-1) + b)  â”‚   â”‚
â”‚   â”‚                                                         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚   Where:                                                        â”‚
â”‚   â€¢ r(t)  = reservoir state at time t                           â”‚
â”‚   â€¢ Î±     = leak rate (memory decay control)                    â”‚
â”‚   â€¢ act   = activation function (tanh or relu)                  â”‚
â”‚   â€¢ Win   = input weight matrix                                 â”‚
â”‚   â€¢ W     = reservoir weight matrix                             â”‚
â”‚   â€¢ b     = bias vector                                         â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚ High Î± (â†’1): Fast dynamics, responds quickly to inputs    â”‚ â”‚
â”‚   â”‚ Low Î±  (â†’0): Slow dynamics, long memory of past states    â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Online Learning with RLS

The Recursive Least Squares (RLS) algorithm enables true online learning without storing historical data.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RLS ALGORITHM FLOW                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   For each new sample (x, y):                                   â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚   â”‚ 1. COMPUTE  â”‚  Pz = P Â· z                                   â”‚
â”‚   â”‚    Pz       â”‚  (where z = [r; x; 1] extended state)         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚          â–¼                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚   â”‚ 2. COMPUTE  â”‚  k = Pz / (Î» + z'Â·Pz)                         â”‚
â”‚   â”‚    GAIN k   â”‚  (Kalman-like gain vector)                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚          â–¼                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚   â”‚ 3. UPDATE   â”‚  Wout += k Â· (y - WoutÂ·z)'                    â”‚
â”‚   â”‚    WEIGHTS  â”‚  (gradient descent step)                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚          â–¼                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚   â”‚ 4. UPDATE   â”‚  P = (P - kÂ·Pz') / Î»                          â”‚
â”‚   â”‚    P MATRIX â”‚  (inverse correlation update)                 â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚                                                                 â”‚
â”‚   Î» (forgetting factor):                                        â”‚
â”‚   â€¢ Î» = 1.0: Equal weight to all samples                        â”‚
â”‚   â€¢ Î» < 1.0: More weight to recent samples (concept drift)      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Multi-Horizon Forecasting

The library supports two forecasting strategies:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               MULTI-HORIZON FORECASTING STRATEGIES              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ DIRECT METHOD (useDirectMultiHorizon: true)                 â”‚ â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â”‚   Single forward pass predicts ALL future steps:            â”‚ â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â”‚   Input â”€â”€â–¶ Reservoir â”€â”€â–¶ Wout â”€â”€â–¶ [Å·â‚, Å·â‚‚, Å·â‚ƒ, ..., Å·â‚•] | â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â”‚   âœ… Pros: Fast, no error accumulation                      â”‚ â”‚
â”‚ â”‚   âŒ Cons: Larger output layer, independent step errors     â”‚ â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ RECURSIVE METHOD (useDirectMultiHorizon: false)             â”‚ â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â”‚   Roll forward step by step:                                â”‚ â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â”‚   t+1: Reservoir(x_t) â”€â”€â–¶ Å·â‚                               â”‚ â”‚
â”‚ â”‚   t+2: Reservoir(x_t) â”€â”€â–¶ Å·â‚‚  (reservoir continues)        â”‚ â”‚
â”‚ â”‚   t+3: Reservoir(x_t) â”€â”€â–¶ Å·â‚ƒ  (same x, evolving state)     â”‚ â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â”‚   âœ… Pros: Smaller model, consistent dynamics               â”‚ â”‚
â”‚ â”‚   âŒ Cons: Slower, potential error accumulation             â”‚ â”‚
â”‚ â”‚                                                             â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Configuration

### Complete Configuration Interface

```typescript
interface ESNRegressionConfig {
  // Reservoir Architecture
  maxSequenceLength: number;      // Default: 64
  maxFutureSteps: number;         // Default: 1
  reservoirSize: number;          // Default: 256
  spectralRadius: number;         // Default: 0.9
  leakRate: number;               // Default: 0.3
  inputScale: number;             // Default: 1.0
  biasScale: number;              // Default: 0.1
  reservoirSparsity: number;      // Default: 0.9
  inputSparsity: number;          // Default: 0.0
  activation: "tanh" | "relu";    // Default: "tanh"
  
  // Readout Configuration
  useInputInReadout: boolean;     // Default: true
  useBiasInReadout: boolean;      // Default: true
  useDirectMultiHorizon: boolean; // Default: true
  
  // RLS Training
  readoutTraining: "rls";         // Default: "rls"
  rlsLambda: number;              // Default: 0.999
  rlsDelta: number;               // Default: 1.0
  l2Lambda: number;               // Default: 0.0001
  gradientClipNorm: number;       // Default: 1.0
  
  // Normalization
  normalizationEpsilon: number;   // Default: 1e-8
  normalizationWarmup: number;    // Default: 10
  
  // Outlier Handling
  outlierThreshold: number;       // Default: 3.0
  outlierMinWeight: number;       // Default: 0.1
  
  // Uncertainty Estimation
  residualWindowSize: number;     // Default: 100
  uncertaintyMultiplier: number;  // Default: 1.96
  
  // Initialization
  weightInitScale: number;        // Default: 0.1
  seed: number;                   // Default: 42
  epsilon: number;                // Default: 1e-8
  verbose: boolean;               // Default: false
}
```

---

### 1. Reservoir Parameters

#### `reservoirSize` ğŸ§ 

**The number of neurons in the reservoir.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RESERVOIR SIZE IMPACT                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Small (64-128)        Medium (256-512)      Large (1024+)   â”‚
â”‚  â”œâ”€â”€ Fast training     â”œâ”€â”€ Good balance      â”œâ”€â”€ High        â”‚
â”‚  â”œâ”€â”€ Low memory        â”œâ”€â”€ Most use cases    â”‚   capacity    â”‚
â”‚  â””â”€â”€ Limited capacity  â””â”€â”€ Recommended       â”œâ”€â”€ Slow        â”‚
â”‚                                              â””â”€â”€ Memory      â”‚
â”‚                                                  intensive   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Optimization Guide:**

| Scenario | Recommended Size | Rationale |
|----------|-----------------|-----------|
| Simple patterns, few features | 64-128 | Avoid overfitting |
| Standard time series | 256-512 | Good generalization |
| Complex, multi-scale patterns | 512-1024 | Capture rich dynamics |
| High-dimensional inputs | 2-4x input dimension | Sufficient expansion |

```typescript
// Example: Simple univariate forecasting
const simple = new ESNRegression({
  reservoirSize: 128,
  // ...
});

// Example: Complex multivariate with many patterns
const complex = new ESNRegression({
  reservoirSize: 512,
  // ...
});
```

---

#### `spectralRadius` ğŸ“Š

**Controls the "echo" property - how long information persists in the reservoir.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SPECTRAL RADIUS EFFECT ON MEMORY                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Memory Length                                               â”‚
â”‚       â”‚                                                      â”‚
â”‚  Long â”‚                         â•­â”€â”€â”€â”€â—                       â”‚
â”‚       â”‚                    â•­â”€â”€â”€â”€â•¯                            â”‚
â”‚       â”‚               â•­â”€â”€â”€â”€â•¯                                 â”‚
â”‚       â”‚          â•­â”€â”€â”€â”€â•¯                                      â”‚
â”‚  Shortâ”‚     â—â”€â”€â”€â”€â•¯                                           â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶        â”‚
â”‚            0.5   0.7   0.9   0.95   0.99                     â”‚
â”‚                  Spectral Radius                             â”‚
â”‚                                                              â”‚
â”‚  âš ï¸  MUST BE < 1.0 for stability (Echo State Property)       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Optimization Guide:**

| Pattern Type | Recommended Range | Example |
|--------------|-------------------|---------|
| Short-term dependencies | 0.5 - 0.7 | High-frequency trading |
| Medium-term patterns | 0.8 - 0.95 | Daily sales forecasting |
| Long-term dependencies | 0.95 - 0.99 | Climate, seasonal data |

```typescript
// Short memory for reactive systems
const reactive = new ESNRegression({
  spectralRadius: 0.6,
  leakRate: 0.8,  // Pair with high leak rate
});

// Long memory for seasonal patterns
const seasonal = new ESNRegression({
  spectralRadius: 0.98,
  leakRate: 0.1,  // Pair with low leak rate
});
```

---

#### `leakRate` ğŸ’§

**Controls how quickly the reservoir "forgets" previous states.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LEAK RATE DYNAMICS                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  r(t) = (1-Î±)Â·r(t-1) + Î±Â·f(input)                            â”‚
â”‚         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚         Memory term     Input term                           â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Î± â†’ 0: Heavy memory, slow response to new inputs       â”‚  â”‚
â”‚  â”‚ Î± â†’ 1: Fast adaptation, quick response (less memory)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  Effective Memory â‰ˆ 1 / (1 - spectralRadius Ã— (1 - Î±))       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Optimization Guide:**

| Scenario | Leak Rate | Spectral Radius | Effect |
|----------|-----------|-----------------|--------|
| Fast-changing signals | 0.7 - 1.0 | 0.7 - 0.9 | Quick adaptation |
| Slow trends | 0.1 - 0.3 | 0.9 - 0.99 | Long memory |
| Mixed frequencies | 0.3 - 0.5 | 0.85 - 0.95 | Balanced |

```typescript
// Fast adaptation for volatile data
const volatile = new ESNRegression({
  leakRate: 0.9,
  spectralRadius: 0.7,
});

// Smooth, trend-following
const trending = new ESNRegression({
  leakRate: 0.2,
  spectralRadius: 0.95,
});
```

---

#### `inputScale` & `biasScale` ğŸ“

**Control the magnitude of input and bias influence on reservoir dynamics.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SCALING EFFECTS                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚Pre-activation = inputScale Ã— Win Ã— x + W Ã— r + biasScale Ã— b â”‚
â”‚                 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                 Input influence       Recurrence  Baseline   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ High inputScale: Reservoir strongly driven by inputs    â”‚ â”‚
â”‚  â”‚ Low inputScale:  Reservoir dominated by internal        â”‚ â”‚
â”‚  â”‚                  dynamics (autonomous behavior)         â”‚ â”‚
â”‚  â”‚                                                         â”‚ â”‚
â”‚  â”‚ biasScale: Adds diversity to neuron firing thresholds   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Optimization Guide:**

```typescript
// Strong input influence (data-driven)
const inputDriven = new ESNRegression({
  inputScale: 2.0,
  biasScale: 0.1,
});

// Balanced dynamics
const balanced = new ESNRegression({
  inputScale: 1.0,
  biasScale: 0.2,
});

// Autonomous reservoir (rich internal dynamics)
const autonomous = new ESNRegression({
  inputScale: 0.5,
  biasScale: 0.3,
});
```

---

#### `reservoirSparsity` & `inputSparsity` ğŸ•¸ï¸

**Control the connectivity density of weight matrices.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SPARSITY VISUALIZATION                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  reservoirSparsity = 0.0 (Dense)   reservoirSparsity = 0.9   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ â— â— â— â— â— â— â— â— â— â— â”‚           â”‚ â—     â—         â—   â”‚   â”‚
â”‚  â”‚ â— â— â— â— â— â— â— â— â— â— â”‚           â”‚     â—     â—         â”‚   â”‚
â”‚  â”‚ â— â— â— â— â— â— â— â— â— â— â”‚           â”‚ â—         â—     â—   â”‚   â”‚
â”‚  â”‚ â— â— â— â— â— â— â— â— â— â— â”‚           â”‚       â—       â—     â”‚   â”‚
â”‚  â”‚ â— â— â— â— â— â— â— â— â— â— â”‚           â”‚ â—   â—     â—         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  100% connections                  10% connections           â”‚
â”‚  Slow, memory heavy                Fast, diverse dynamics    â”‚
â”‚                                                              â”‚
â”‚  âš ï¸ Higher sparsity = fewer connections = faster but may     â”‚
â”‚     lose expressiveness for small reservoirs                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Optimization Guide:**

| Reservoir Size | Recommended Sparsity | Rationale |
|----------------|---------------------|-----------|
| < 100 | 0.0 - 0.5 | Maintain connectivity |
| 100 - 500 | 0.7 - 0.9 | Balance speed & richness |
| > 500 | 0.9 - 0.95 | Computational efficiency |

```typescript
// Small reservoir, keep dense
const smallDense = new ESNRegression({
  reservoirSize: 64,
  reservoirSparsity: 0.3,
});

// Large sparse reservoir
const largeSparse = new ESNRegression({
  reservoirSize: 1024,
  reservoirSparsity: 0.95,
});
```

---

#### `activation` âš¡

**The nonlinear activation function for reservoir neurons.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ACTIVATION FUNCTIONS                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  "tanh" (default)              "relu"                        â”‚
â”‚       â”‚                             â”‚                        â”‚
â”‚    1 â”€â”¤     â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€          1 â”€â”¤           /             â”‚
â”‚       â”‚    â•±                       â”‚         /               â”‚
â”‚    0 â”€â”¼â”€â”€â”€â—                     0 â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â—                â”‚
â”‚       â”‚    â•²                       â”‚                         â”‚
â”‚   -1 â”€â”¤     â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€         -1 â”€â”¤                         â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚       -3  -1   0   1   3           -1   0   1   2   3        â”‚
â”‚                                                              â”‚
â”‚  tanh:                         relu:                         â”‚
â”‚  âœ… Bounded output [-1, 1]     âœ… Faster computation         â”‚
â”‚  âœ… Smooth gradients           âœ… Sparse activations         â”‚
â”‚  âœ… Default choice             âŒ Unbounded (may explode)    â”‚
â”‚  âŒ Saturation at extremes     âŒ Dead neurons possible      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```typescript
// Standard choice (recommended)
const standard = new ESNRegression({
  activation: "tanh",
});

// For non-negative patterns, experiment with relu
const reluReservoir = new ESNRegression({
  activation: "relu",
  spectralRadius: 0.7,  // Lower for stability
  inputScale: 0.5,      // Reduce to prevent explosion
});
```

---

### 2. Training Parameters

#### `rlsLambda` (Forgetting Factor) ğŸ”„

**Controls how much weight is given to recent vs. older samples.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FORGETTING FACTOR EFFECT                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Weight of sample at time (t - k):  w(k) = Î»áµ                â”‚
â”‚                                                              â”‚
â”‚  Weight                                                      â”‚
â”‚    â”‚                                                         â”‚
â”‚  1 â”¤â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—â”€â”€â—   Î» = 1.0 (no forgetting)            â”‚
â”‚    â”‚                                                         â”‚
â”‚    â”‚â—                                                        â”‚
â”‚ .5 â”¤  â—                                                      â”‚
â”‚    â”‚    â—â”€â”€â—â”€â”€â—          Î» = 0.99 (slow forgetting)          â”‚
â”‚    â”‚                                                         â”‚
â”‚    â”‚â—                                                        â”‚
â”‚  0 â”¤  â—â”€â”€â—â”€â”€â—â”€â”€â—         Î» = 0.95 (fast forgetting)          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶                                      â”‚
â”‚      t  t-5  t-10  t-15   Samples ago                        â”‚
â”‚                                                              â”‚
â”‚  Effective memory â‰ˆ 1 / (1 - Î»)                              â”‚
â”‚  Î» = 0.999 â†’ ~1000 samples effective memory                  â”‚
â”‚  Î» = 0.99  â†’ ~100 samples effective memory                   â”‚
â”‚  Î» = 0.95  â†’ ~20 samples effective memory                    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Optimization Guide:**

| Scenario | rlsLambda | Effective Memory |
|----------|-----------|------------------|
| Stationary data | 0.9999 - 1.0 | Very long |
| Slow concept drift | 0.999 - 0.9995 | ~1000-2000 samples |
| Moderate drift | 0.99 - 0.999 | ~100-1000 samples |
| Fast-changing dynamics | 0.95 - 0.99 | ~20-100 samples |

```typescript
// Stationary environment
const stationary = new ESNRegression({
  rlsLambda: 0.9999,
});

// Adapting to concept drift
const adaptive = new ESNRegression({
  rlsLambda: 0.995,  // ~200 samples effective memory
});
```

---

#### `rlsDelta` (Initial P Scale) ğŸ“

**Controls the initial confidence/uncertainty in weight estimates.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RLS DELTA INITIALIZATION                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Pâ‚€ = (1/Î´) Ã— I   (Identity matrix scaled)                   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Small Î´ (0.01-0.1): Large initial P                     â”‚ â”‚
â”‚  â”‚   â†’ High initial learning rate                          â”‚ â”‚
â”‚  â”‚   â†’ Fast initial adaptation                             â”‚ â”‚
â”‚  â”‚   â†’ Risk of instability                                 â”‚ â”‚
â”‚  â”‚                                                         â”‚ â”‚
â”‚  â”‚ Large Î´ (1.0-10.0): Small initial P                     â”‚ â”‚
â”‚  â”‚   â†’ Conservative initial learning                       â”‚ â”‚
â”‚  â”‚   â†’ Stable but slower convergence                       â”‚ â”‚
â”‚  â”‚   â†’ Recommended for most cases                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```typescript
// Conservative (recommended default)
const conservative = new ESNRegression({
  rlsDelta: 1.0,
});

// Aggressive initial learning
const aggressive = new ESNRegression({
  rlsDelta: 0.1,
});
```

---

#### `l2Lambda` (Regularization) ğŸ›¡ï¸

**L2 regularization strength to prevent overfitting.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  L2 REGULARIZATION                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Loss = MSE + l2Lambda Ã— ||Wout||Â²                           â”‚
â”‚                                                              â”‚
â”‚  Effect: Weight decay towards zero                           â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ l2Lambda = 0:      No regularization                    â”‚ â”‚
â”‚  â”‚ l2Lambda = 1e-4:   Light regularization (default)       â”‚ â”‚
â”‚  â”‚ l2Lambda = 1e-3:   Moderate regularization              â”‚ â”‚
â”‚  â”‚ l2Lambda = 1e-2:   Strong regularization                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  âš ï¸ Too high: Underfitting, poor predictions                 â”‚
â”‚  âš ï¸ Too low:  Overfitting, unstable weights                  â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```typescript
// Standard regularization
const standard = new ESNRegression({
  l2Lambda: 0.0001,
});

// High regularization for noisy data
const noisy = new ESNRegression({
  l2Lambda: 0.001,
});
```

---

#### `gradientClipNorm` âœ‚ï¸

**Clips weight updates to prevent explosive learning.**

```typescript
// Default clipping
const defaultClip = new ESNRegression({
  gradientClipNorm: 1.0,
});

// More aggressive clipping for unstable data
const conservativeClip = new ESNRegression({
  gradientClipNorm: 0.5,
});

// Disable clipping (not recommended)
const noClip = new ESNRegression({
  gradientClipNorm: 0,  // Disabled when <= 0
});
```

---

### 3. Normalization Parameters

#### `normalizationWarmup` ğŸ”¥

**Number of samples before activating normalization.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WELFORD ONLINE NORMALIZATION                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Phase 1: Warmup (samples < warmup)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ Collect statistics only                               â”‚ â”‚
â”‚  â”‚ â€¢ No normalization applied                              â”‚ â”‚
â”‚  â”‚ â€¢ Accumulate mean and variance estimates                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  Phase 2: Active (samples >= warmup)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ Apply normalization: x_norm = (x - Î¼) / Ïƒ             â”‚ â”‚
â”‚  â”‚ â€¢ Continue updating statistics with each sample         â”‚ â”‚
â”‚  â”‚ â€¢ Welford's algorithm for numerical stability           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```typescript
// Quick warmup for stationary data
const quickWarmup = new ESNRegression({
  normalizationWarmup: 5,
});

// Longer warmup for varying statistics
const longerWarmup = new ESNRegression({
  normalizationWarmup: 50,
});
```

---

### 4. Prediction Parameters

#### `maxFutureSteps` ğŸ”®

**Maximum number of future steps the model can predict.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MULTI-STEP PREDICTION                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  With useDirectMultiHorizon = true:                          â”‚
â”‚                                                              â”‚
â”‚  Training Y shape: [nSamples][nTargets Ã— maxFutureSteps]     â”‚
â”‚                                                              â”‚
â”‚  Example: 2 targets, 3 future steps                          â”‚
â”‚  Y[i] = [y1_t+1, y2_t+1, y1_t+2, y2_t+2, y1_t+3, y2_t+3]     â”‚
â”‚          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚            Step 1          Step 2         Step 3             â”‚
â”‚                                                              â”‚
â”‚  Prediction output:                                          â”‚
â”‚  predict(3).predictions = [                                  â”‚
â”‚    [y1_t+1, y2_t+1],  // Step 1                              â”‚
â”‚    [y1_t+2, y2_t+2],  // Step 2                              â”‚
â”‚    [y1_t+3, y2_t+3],  // Step 3                              â”‚
â”‚  ]                                                           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```typescript
// Single-step prediction
const singleStep = new ESNRegression({
  maxFutureSteps: 1,
});

// Multi-step with 5-step horizon
const multiStep = new ESNRegression({
  maxFutureSteps: 5,
  useDirectMultiHorizon: true,
});

// Training data for multiStep (2 features, 1 target, 5 steps)
const xTrain = [[1, 2], [3, 4], [5, 6]];
const yTrain = [
  [10, 11, 12, 13, 14],  // targets for t+1 through t+5
  [20, 21, 22, 23, 24],
  [30, 31, 32, 33, 34],
];
```

---

#### `uncertaintyMultiplier` ğŸ“ˆ

**Multiplier for confidence interval width.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CONFIDENCE INTERVALS                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Bounds = prediction Â± (uncertaintyMultiplier Ã— std)         â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Multiplier â”‚ Coverage (Normal dist)                     â”‚ â”‚
â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚
â”‚  â”‚    1.00    â”‚ ~68% (1 standard deviation)                â”‚ â”‚
â”‚  â”‚    1.64    â”‚ ~90%                                       â”‚ â”‚
â”‚  â”‚    1.96    â”‚ ~95% (default, 2Ïƒ)                         â”‚ â”‚
â”‚  â”‚    2.58    â”‚ ~99%                                       â”‚ â”‚
â”‚  â”‚    3.00    â”‚ ~99.7%                                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```typescript
// 95% confidence (default)
const ci95 = new ESNRegression({
  uncertaintyMultiplier: 1.96,
});

// 99% confidence (wider intervals)
const ci99 = new ESNRegression({
  uncertaintyMultiplier: 2.58,
});
```

---

#### `outlierThreshold` & `outlierMinWeight` ğŸ¯

**Control outlier detection and downweighting during training.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OUTLIER HANDLING                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Z-score = |residual - mean| / std                           â”‚
â”‚                                                              â”‚
â”‚  Weight calculation:                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ if Z â‰¤ threshold:  weight = 1.0                         â”‚ â”‚
â”‚  â”‚ if Z > threshold:  weight = max(threshold/Z, minWeight) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  Weight                                                      â”‚
â”‚    â”‚                                                         â”‚
â”‚  1 â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚    â”‚            â”‚                                            â”‚
â”‚    â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚
â”‚  .1â”¤                          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (minWeight floor)  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶                   â”‚
â”‚         threshold            Z-score                         â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```typescript
// Aggressive outlier rejection
const aggressiveOutlier = new ESNRegression({
  outlierThreshold: 2.0,    // Tighter threshold
  outlierMinWeight: 0.05,   // Stronger downweighting
});

// Permissive (keep more data)
const permissive = new ESNRegression({
  outlierThreshold: 4.0,    // Wider threshold
  outlierMinWeight: 0.3,    // Less downweighting
});
```

---

## ğŸ“Š API Reference

### Constructor

```typescript
const esn = new ESNRegression(config?: Partial<ESNRegressionConfig>);
```

### Methods

#### `fitOnline({ xCoordinates, yCoordinates }): FitResult`

Train the model incrementally with new data.

```typescript
interface FitResult {
  samplesProcessed: number;   // Number of samples in this batch
  averageLoss: number;        // Mean squared error
  gradientNorm: number;       // Magnitude of weight updates
  driftDetected: boolean;     // Concept drift indicator
  sampleWeight: number;       // Last sample's outlier weight
}

// Example
const result = esn.fitOnline({
  xCoordinates: [[1, 2], [3, 4]],
  yCoordinates: [[10], [20]],
});
```

---

#### `predict(futureSteps: number): PredictionResult`

Generate predictions for future time steps.

```typescript
interface PredictionResult {
  predictions: number[][];   // [step][target] predicted values
  lowerBounds: number[][];   // Lower confidence bounds
  upperBounds: number[][];   // Upper confidence bounds
  confidence: number;        // Overall confidence score [0, 1]
}

// Example
const pred = esn.predict(3);
console.log(pred.predictions[0]);  // First step predictions
console.log(pred.lowerBounds[0]);  // First step lower bounds
```

---

#### `getModelSummary(): ModelSummary`

Get model architecture information.

```typescript
interface ModelSummary {
  totalParameters: number;      // Trainable parameter count
  receptiveField: number;       // Effective memory length
  spectralRadius: number;       // Current spectral radius
  reservoirSize: number;        // Number of reservoir neurons
  nFeatures: number;            // Input dimension
  nTargets: number;             // Output dimension
  maxSequenceLength: number;    // Ring buffer capacity
  maxFutureSteps: number;       // Maximum forecast horizon
  sampleCount: number;          // Total samples trained
  useDirectMultiHorizon: boolean;
}
```

---

#### `getWeights(): WeightInfo`

Retrieve all weight matrices.

```typescript
interface WeightInfo {
  weights: Array<{
    name: string;      // "Win", "W", "bias", "Wout"
    shape: number[];   // Dimensions
    values: number[];  // Flattened values
  }>;
}
```

---

#### `getNormalizationStats(): NormalizationStats`

Get normalization parameters.

```typescript
interface NormalizationStats {
  means: number[];    // Per-feature means
  stds: number[];     // Per-feature standard deviations
  count: number;      // Samples observed
  isActive: boolean;  // Whether normalization is active
}
```

---

#### `save(): string`

Serialize the model to JSON.

```typescript
const serialized = esn.save();
localStorage.setItem("model", serialized);
```

---

#### `load(data: string): void`

Deserialize a saved model.

```typescript
const serialized = localStorage.getItem("model");
esn.load(serialized!);
```

---

#### `reset(): void`

Reset the model to initial state.

```typescript
esn.reset();  // Clears all learned weights and statistics
```

---

## ğŸ¯ Use Cases & Optimization

### ğŸ“ˆ Stock Price Prediction

```typescript
const stockPredictor = new ESNRegression({
  reservoirSize: 512,
  spectralRadius: 0.95,        // Long memory for trends
  leakRate: 0.2,               // Slow dynamics
  maxFutureSteps: 5,           // 5-day forecast
  rlsLambda: 0.998,            // Adapt to market changes
  outlierThreshold: 2.5,       // Handle volatility
  useDirectMultiHorizon: true,
});

// Features: [open, high, low, close, volume]
// Target: next 5 days' closing prices
```

---

### ğŸŒ¡ï¸ Sensor Time Series

```typescript
const sensorModel = new ESNRegression({
  reservoirSize: 256,
  spectralRadius: 0.85,        // Medium memory
  leakRate: 0.5,               // Moderate adaptation
  maxFutureSteps: 10,          // 10-step prediction
  inputScale: 0.5,             // Sensors often pre-scaled
  normalizationWarmup: 100,    // Let stats stabilize
  residualWindowSize: 200,     // Larger window for stability
});
```

---

### ğŸ“Š Demand Forecasting

```typescript
const demandForecaster = new ESNRegression({
  reservoirSize: 384,
  spectralRadius: 0.92,
  leakRate: 0.3,
  maxFutureSteps: 7,           // Weekly forecast
  rlsLambda: 0.9995,           // Stable patterns
  l2Lambda: 0.0005,            // Some regularization
  useDirectMultiHorizon: true,
});

// Features: [day_of_week, promotions, weather, ...]
// Target: daily demand for next 7 days
```

---

### âš¡ Real-Time Signal Processing

```typescript
const signalProcessor = new ESNRegression({
  reservoirSize: 128,          // Small for speed
  spectralRadius: 0.7,         // Fast dynamics
  leakRate: 0.9,               // Quick adaptation
  maxFutureSteps: 1,           // Single-step prediction
  reservoirSparsity: 0.95,     // Very sparse for speed
  rlsLambda: 0.99,             // Fast forgetting
  normalizationWarmup: 5,      // Quick startup
});
```

---

### ğŸ›ï¸ Parameter Optimization Cheatsheet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  QUICK OPTIMIZATION GUIDE                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Problem                        Solution                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  Underfitting                   â†‘ reservoirSize, â†“ l2Lambda      â”‚
â”‚  Overfitting                    â†“ reservoirSize, â†‘ l2Lambda      â”‚
â”‚  Slow convergence               â†“ rlsDelta, â†“ rlsLambda          â”‚
â”‚  Unstable training              â†“ gradientClipNorm, â†‘ l2Lambda   â”‚
â”‚  Missing long patterns          â†‘ spectralRadius, â†“ leakRate     â”‚
â”‚  Too slow response              â†“ spectralRadius, â†‘ leakRate     â”‚
â”‚  Noisy predictions              â†‘ l2Lambda, â†‘ residualWindowSize â”‚
â”‚  Concept drift                  â†“ rlsLambda (0.99-0.999)         â”‚
â”‚  Memory issues                  â†‘ reservoirSparsity, â†“ reservoir â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¾ Model Persistence

### Save and Load Example

```typescript
import { ESNRegression } from "jsr:@hviana/multivariate-regression";

// Create and train model
const esn = new ESNRegression({
  reservoirSize: 256,
  maxFutureSteps: 3,
  seed: 42,  // Important: must match when loading
});

// Train with data
esn.fitOnline({
  xCoordinates: trainingX,
  yCoordinates: trainingY,
});

// Save model
const modelJson = esn.save();
// Store to file, database, localStorage, etc.
await Deno.writeTextFile("model.json", modelJson);

// Later: Load model
const loadedJson = await Deno.readTextFile("model.json");

// Create new instance with SAME config
const loadedEsn = new ESNRegression({
  reservoirSize: 256,
  maxFutureSteps: 3,
  seed: 42,  // Must match original
});

loadedEsn.load(loadedJson);

// Continue training or predict
const prediction = loadedEsn.predict(3);
```

---

## ğŸ”¬ Advanced Topics

### Understanding the Extended State Vector

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXTENDED STATE VECTOR z                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  z = [ râ‚, râ‚‚, ..., râ‚™ | xâ‚, xâ‚‚, ..., xâ‚˜ | 1 ]               â”‚
â”‚       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€                â”‚
â”‚       Reservoir state    Input (optional)   Bias             â”‚
â”‚       (N neurons)        (F features)       (optional)       â”‚
â”‚                                                              â”‚
â”‚  Configuration:                                              â”‚
â”‚  â€¢ useInputInReadout = true:  Include input features         â”‚
â”‚  â€¢ useBiasInReadout = true:   Include bias term              â”‚
â”‚                                                              â”‚
â”‚  zDim = N + (useInputInReadout ? F : 0)                      â”‚
â”‚           + (useBiasInReadout ? 1 : 0)                       â”‚
â”‚                                                              â”‚
â”‚  Example: N=256, F=10, both enabled                          â”‚
â”‚  zDim = 256 + 10 + 1 = 267                                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Memory Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INTERNAL DATA FLOW                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Input X â”€â”€â”¬â”€â”€â–¶ RingBuffer (stores last maxSequenceLength)  â”‚
â”‚            â”‚                                                 â”‚
â”‚            â”œâ”€â”€â–¶ WelfordNormalizer â”€â”€â–¶ xNorm                â”‚
â”‚            â”‚                                                 â”‚
â”‚            â””â”€â”€â–¶ ESNReservoir â”€â”€â–¶ r (state)                 â”‚
â”‚                                   â”‚                          â”‚
â”‚                                   â–¼                          â”‚
â”‚                    z = [r; xNorm; 1] (extended state)        â”‚
â”‚                                   â”‚                          â”‚
â”‚                                   â–¼                          â”‚
â”‚                    LinearReadout (Wout) â”€â”€â–¶ Å·               â”‚
â”‚                                   â”‚                          â”‚
â”‚                                   â–¼                          â”‚
â”‚                    ResidualStats â”€â”€â–¶ Confidence bounds      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Deterministic Behavior

The library guarantees deterministic results:

```typescript
// Same configuration + same seed = identical results
const esn1 = new ESNRegression({ seed: 12345 });
const esn2 = new ESNRegression({ seed: 12345 });

esn1.fitOnline({ xCoordinates: data, yCoordinates: labels });
esn2.fitOnline({ xCoordinates: data, yCoordinates: labels });

const pred1 = esn1.predict(3);
const pred2 = esn2.predict(3);

// pred1.predictions === pred2.predictions  âœ…
```

---

## ğŸ› ï¸ Troubleshooting

### Common Issues

| Issue | Possible Cause | Solution |
|-------|---------------|----------|
| `NaN` in predictions | Exploding gradients | â†“ `inputScale`, â†“ `spectralRadius`, â†‘ `gradientClipNorm` |
| Poor accuracy | Underfitting | â†‘ `reservoirSize`, tune `spectralRadius`/`leakRate` |
| Slow training | Large reservoir | â†‘ `reservoirSparsity`, â†“ `reservoirSize` |
| Config mismatch on load | Different settings | Ensure identical config when loading |
| Memory errors | Too large reservoir | â†“ `reservoirSize`, â†‘ `sparsity` |

---

## ğŸ“ License

MIT License Â© 2025 [Henrique Emanoel Viana](https://github.com/hviana)

---

<div align="center">

**[â¬† Back to Top](#-esnregression---echo-state-network-for-multivariate-time-series)**

Made with â¤ï¸ for the time series community

[GitHub](https://github.com/hviana/multivariate-regression) â€¢ [JSR Package](https://jsr.io/@hviana/multivariate-regression)

</div>