Create a TypeScript library implementing an Echo State Network (ESN) for multivariate autoregressive regression with online learning capabilities. The implementation must prioritize low memory allocation and low CPU usage through buffer reuse, sparse matrix operations, and avoiding unnecessary object creation.

===========================================
LIBRARY NAME AND EXPORT
===========================================

Export a class named ESNRegression as the main entry point and default export.

===========================================
PUBLIC INTERFACES
===========================================

Define and export the following interfaces:

1. ESNRegressionConfig
   - reservoirSize: number - Size of the reservoir (hidden state dimension)
   - spectralRadius: number - Target spectral radius for reservoir weight matrix
   - leakRate: number - Leaky integration rate (0 to 1)
   - inputScale: number - Scaling factor for input weights
   - biasScale: number - Scaling factor for bias vector
   - reservoirSparsity: number - Fraction of zero elements in reservoir matrix (0 to 1)
   - inputSparsity: number - Fraction of zero elements in input matrix (0 to 1)
   - activation: "tanh" | "relu" - Activation function type
   - useInputInReadout: boolean - Include raw input in extended state
   - useBiasInReadout: boolean - Include bias term in extended state
   - readoutTraining: "rls" - Training algorithm (only RLS supported)
   - rlsLambda: number - RLS forgetting factor
   - rlsDelta: number - RLS initial covariance scaling
   - epsilon: number - Small constant for numerical stability
   - l2Lambda: number - L2 regularization coefficient
   - gradientClipNorm: number - Maximum gradient norm for clipping
   - normalizationEpsilon: number - Epsilon for normalization stability
   - normalizationWarmup: number - Samples before normalization activates
   - outlierThreshold: number - Z-score threshold for outlier detection
   - outlierMinWeight: number - Minimum sample weight for outliers
   - uncertaintyMultiplier: number - Confidence interval multiplier (e.g., 1.96 for 95%)
   - weightInitScale: number - Standard deviation for weight initialization
   - seed: number - Random seed for reproducibility
   - verbose: boolean - Enable verbose logging

2. FitResult
   - samplesProcessed: number - Number of samples processed in this call
   - averageLoss: number - Mean squared error over processed samples
   - gradientNorm: number - Norm of gradient from last update
   - driftDetected: boolean - Whether concept drift was detected
   - sampleWeight: number - Weight applied to last sample (outlier detection)

3. PredictionResult
   - predictions: number[][] - Array of predicted coordinate vectors
   - lowerBounds: number[][] - Lower confidence bounds per prediction
   - upperBounds: number[][] - Upper confidence bounds per prediction
   - confidence: number - Overall prediction confidence score (0 to 1)

4. ModelSummary
   - totalParameters: number - Total trainable and fixed parameters
   - receptiveField: number - Effective temporal memory length
   - spectralRadius: number - Configured spectral radius
   - reservoirSize: number - Reservoir dimension
   - nFeatures: number - Input/output feature dimension
   - nTargets: number - Number of prediction targets
   - sampleCount: number - Total samples seen during training

5. WeightInfo
   - weights: Array<{ name: string; shape: number[]; values: number[] }> - All weight matrices

6. NormalizationStats
   - means: number[] - Running mean per feature
   - stds: number[] - Running standard deviation per feature
   - count: number - Number of samples accumulated
   - isActive: boolean - Whether normalization has warmed up

===========================================
DEFAULT CONFIGURATION VALUES
===========================================

reservoirSize: 256
spectralRadius: 0.9
leakRate: 0.3
inputScale: 1.0
biasScale: 0.1
reservoirSparsity: 0.9
inputSparsity: 0.0
activation: "tanh"
useInputInReadout: true
useBiasInReadout: true
readoutTraining: "rls"
rlsLambda: 0.999
rlsDelta: 1.0
epsilon: 1e-8
l2Lambda: 0.0001
gradientClipNorm: 1.0
normalizationEpsilon: 1e-8
normalizationWarmup: 10
outlierThreshold: 3.0
outlierMinWeight: 0.1
uncertaintyMultiplier: 1.96
weightInitScale: 0.1
seed: 42
verbose: false

===========================================
PUBLIC METHODS OF ESNRegression CLASS
===========================================

1. constructor(config?: Partial<ESNRegressionConfig>)
   - Accepts partial configuration, merges with defaults
   - Does NOT allocate model buffers (lazy initialization)

2. fitOnline(params: { coordinates: number[][] }): FitResult
   - Primary training method for online learning
   - Accepts 2D array where each row is a coordinate vector
   - Requires at least 2 rows (forms input-target pairs)
   - Initializes model on first call based on feature dimension
   - Processes samples sequentially, updating reservoir state and readout weights
   - Returns training metrics

3. predict(futureSteps: number): PredictionResult
   - Autoregressive multi-step prediction
   - Uses current reservoir state and latest coordinates
   - Rolls forward using predictions as next inputs
   - Computes uncertainty bounds that grow with prediction horizon
   - Requires model to be initialized first

4. getModelSummary(): ModelSummary
   - Returns summary statistics about model architecture

5. getWeights(): WeightInfo
   - Returns all weight matrices (Wout, Win, W, bias)

6. getNormalizationStats(): NormalizationStats
   - Returns current normalization statistics

7. reset(): void
   - Resets model to uninitialized state
   - Clears all learned weights and statistics

8. save(): string
   - Serializes complete model state to JSON string
   - Includes all weights, optimizer state, normalizer state

9. load(str: string): void
   - Deserializes model state from JSON string
   - Restores all internal state for continued training or prediction

===========================================
INTERNAL ARCHITECTURE REQUIREMENTS
===========================================

MEMORY OPTIMIZATION STRATEGIES:

1. Buffer Reuse
   - Pre-allocate all working buffers during initialization
   - Reuse scratch arrays for temporary computations
   - Avoid allocating arrays inside loops or hot paths

2. Typed Arrays
   - Use Float64Array for all numerical data
   - Use Int32Array for sparse matrix indices
   - Avoid using standard JavaScript arrays for numerical operations

3. Buffer Pool Pattern
   - Implement a BufferPool class that caches and reuses Float64Array instances
   - Pool buffers by size for efficient reuse

4. Tensor Arena
   - Single contiguous buffer for related tensors
   - Named allocation with offset tracking

CPU OPTIMIZATION STRATEGIES:

1. Sparse Matrix Operations
   - Store sparse matrices using coordinate format (row indices, column indices)
   - Implement sparse matrix-vector multiplication that only touches non-zero elements
   - Use sparsity for both reservoir weights and input weights

2. Loop Optimization
   - Inline small helper functions
   - Avoid function calls in tight loops
   - Use direct array indexing instead of iterators

3. Numerical Stability
   - Clamp values to prevent overflow/underflow
   - Check for non-finite values and sanitize
   - Use epsilon values to prevent division by zero

4. Activation Functions
   - Implement with early exit for extreme values (tanh saturates beyond ±20)
   - Process arrays in-place when possible

===========================================
INTERNAL CLASSES TO IMPLEMENT
===========================================

1. TensorShape
   - Immutable dimension descriptor
   - Computes strides and total size

2. TensorView
   - View into Float64Array with offset
   - get/set/fill operations

3. BufferPool
   - Acquires and releases Float64Array buffers
   - Limits pool size to prevent memory bloat

4. TensorArena
   - Single buffer with named sub-allocations
   - Reset clears all allocations

5. TensorOps (static methods)
   - matVec: Dense matrix-vector multiplication
   - dot: Vector dot product
   - axpy: y += alpha * x operation
   - copy: Array copy with offsets
   - norm: Euclidean norm
   - fill: Fill array region
   - sparseMatVecByIndices: Sparse matrix-vector using index arrays
   - clampInPlace: Clamp values to range
   - hasNonFinite: Check for NaN/Infinity
   - sanitize: Replace non-finite values

6. RandomGenerator
   - Xorshift32 PRNG for reproducibility
   - uniform(): [0, 1)
   - normal(): Standard normal via Box-Muller
   - range(min, max): Uniform in range
   - getState/setState for serialization

7. ActivationOps (static methods)
   - tanh: Hyperbolic tangent with saturation handling
   - relu: Rectified linear unit
   - apply: Dispatch based on type string

8. WelfordAccumulator
   - Single-pass mean and variance computation
   - Numerically stable online algorithm

9. WelfordNormalizer
   - Per-feature normalization using Welford's algorithm
   - Tracks min/max observed values for output clamping
   - Warmup period before activation
   - normalize/denormalize methods

10. ResidualStatsTracker
    - Tracks prediction residual statistics per target
    - Used for uncertainty estimation

11. OutlierDownweighter
    - Computes sample weights based on z-score
    - Exponential downweighting beyond threshold

12. LossFunction (static)
    - mse: Mean squared error computation

13. MetricsAccumulator
    - Accumulates loss, tracks gradient norm
    - Provides running averages

14. SparseIndices
    - Stores row/column indices for sparse matrix
    - Constructor generates random sparsity pattern
    - Serialization support

15. SpectralRadiusScaler (static)
    - Power iteration to estimate spectral radius
    - Scales matrix to target spectral radius

16. ESNReservoir
    - Manages reservoir weights (W), input weights (Win), bias
    - Sparse storage and operations
    - update(): Process input, update internal state
    - updateScratch(): Update using external state buffer (for prediction)
    - Serialization support

17. RLSState
    - Stores covariance matrix P for RLS algorithm
    - Tracks update count

18. RLSOptimizer
    - Recursive Least Squares weight updates
    - Periodic symmetrization of P matrix
    - Trace monitoring for stability
    - L2 regularization support
    - Gradient clipping

19. LinearReadout
    - Output weights (Wout)
    - buildExtendedState(): Concatenate reservoir state, input, bias
    - forward(): Compute output predictions

20. SerializationHelper (static)
    - JSON serialize/deserialize wrappers

===========================================
ALGORITHM FLOW
===========================================

TRAINING (fitOnline):
1. Validate input dimensions
2. Initialize model on first call (lazy initialization)
3. For each consecutive pair (input, target):
   a. Update normalizer statistics with input
   b. Normalize input
   c. Update reservoir state
   d. Build extended state (reservoir + optional input + optional bias)
   e. Forward through readout to get prediction
   f. Normalize target
   g. Compute loss
   h. Compute outlier weight based on residual z-score
   i. Update readout weights via RLS
   j. Update residual statistics
4. Store latest coordinates for prediction
5. Return fit metrics

PREDICTION (predict):
1. Validate model is initialized
2. Copy current reservoir state to scratch buffer
3. Start with latest coordinates as input
4. For each future step:
   a. Normalize input
   b. Update scratch reservoir state
   c. Build extended state
   d. Forward through readout
   e. Denormalize output
   f. Sanitize predictions (handle NaN, clamp to reasonable range)
   g. Compute uncertainty bounds (grow with horizon)
   h. Use prediction as next input
5. Compute overall confidence score
6. Return predictions with bounds

===========================================
NUMERICAL STABILITY REQUIREMENTS
===========================================

1. All divisions must check for zero denominator (use epsilon)
2. Check for NaN/Infinity after all operations and sanitize
3. Clamp predictions to observed data range with margin
4. Periodically symmetrize RLS covariance matrix
5. Reset RLS covariance if trace becomes too large or small
6. Clamp normalized values to ±10
7. Clamp raw predictions to ±100 before denormalization
8. Use fallback values when computations produce non-finite results

===========================================
SERIALIZATION REQUIREMENTS
===========================================

1. All numeric arrays serialized as standard JavaScript arrays
2. Config object serialized directly
3. Each internal class implements serialize() and static deserialize()
4. Main save/load uses JSON.stringify/parse
5. After load, reinitialize scratch buffers

===========================================
ERROR HANDLING
===========================================

1. Throw descriptive errors for invalid inputs
2. fitOnline requires coordinates array with at least 2 rows
3. All rows must have consistent dimension
4. predict requires model to be initialized
5. futureSteps must be positive integer
6. Feature dimension must match after initialization

===========================================
CODE STYLE
===========================================

1. Use TypeScript with strict typing
2. Private members prefixed with underscore or use private keyword
3. Readonly where appropriate
4. Object.freeze for immutable structures
5. JSDoc comments for public methods
6. No external dependencies
