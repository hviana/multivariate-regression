Implement a TypeScript library named "ESNRegression": an Echo State Network (ESN) / Reservoir Computing model for multivariate regression with incremental online learning using RLS readout training and Welford z-score normalization.

OUTPUT REQUIREMENTS (STRICT)

* Output ONLY the complete TypeScript implementation (single self-contained module / file).
* No markdown, no explanations, no extra text.
* The code must compile and run without external heavy runtime dependencies.

SCOPE AND GOALS

* Build ONE single self-contained TypeScript library (no heavy runtime deps) that trains online (one sample at a time) and predicts multiple steps ahead.
* Primary use-case is tight CPU and memory environments, so the implementation MUST be deterministic, numerically stable, and allocation-free in hot paths.
* The model MUST learn nonlinear cross-variable relationships and implicit lag structure via a fixed recurrent reservoir state with fading memory within a fixed maximum history cap (maxSequenceLength). History is always capped; truncate instead of allocating.

NON-NEGOTIABLE CRITICAL BEHAVIOR (DO NOT VIOLATE)

1. Latest-X is always internal and authoritative

* fitOnline() MUST push each xCoordinates row into the internal RingBuffer BEFORE any training decision.
* predict() MUST assemble its input window ONLY from the internal RingBuffer; it MUST NOT require the caller to pass the latest X.

2. Correct timestep (no off-by-one)

* For any predict(futureSteps), the input window MUST end at the most recently ingested X (the last pushed row).
* Never use “features.length - 2” or any off-by-one logic; the window MUST include the newest X.

3. Prevent “all prices always increasing” bias (common failure)

* Output head MUST NOT drift to monotonic increase due to recursive feeding mistakes, wrong timestep, or history corruption.
* If useDirectMultiHorizon=true: output [nTargets * maxFutureSteps] from the current reservoir state (and optional current input); DO NOT feed predicted Y back into X.
* If recursive roll-forward is enabled: ONLY when explicitly configured; use a dedicated rollforward scratch window and NEVER mutate the main RingBuffer during predict.
* ResidualStatsTracker and normalization MUST be applied consistently to prevent scale drift and confidence drift.

IMPORTANT CONSTRAINT NOTE (MUST ENFORCE)

* fitOnline does NOT support observe-only.
* Therefore: xCoordinates.length MUST equal yCoordinates.length; otherwise throw Error (no partial ingestion behavior).

MODEL ARCHITECTURE (ESN / RESERVOIR COMPUTING)

Input/shape

* Input is a multivariate time series row x_t with shape [nFeatures] (batch fixed to 1).
* The internal history window has shape [seqLen, nFeatures] assembled from RingBuffer with seqLen <= maxSequenceLength.

Reservoir state

* Maintain a reservoir state vector r_t with shape [reservoirSize].
* Update per timestep using a leaky integrator ESN:
  r_t = (1 - leakRate) * r_{t-1} + leakRate * activation( Win * x_t + W * r_{t-1} + bias )
* activation is tanh by default (or configurable).
* Win (reservoirSize x nFeatures) and W (reservoirSize x reservoirSize) are fixed after initialization (not trained) and MUST be deterministic with seed.
* Enforce echo-state behavior by scaling W to targetSpectralRadius and applying sparse connectivity (optional) at init.
* Optional input scaling and state scaling parameters must be supported (fixed after init).

Readout (trainable)

* Train ONLY a linear readout layer that maps extended state z_t to outputs:
  z_t = [r_t; x_t; 1]  (configurable inclusion of x_t and bias)
  y_hat_t = Wout * z_t
* Wout has shape [nTargets * maxFutureSteps, zDim] for direct multi-horizon output (preferred default).
* Alternative: for single-step output, Wout has shape [nTargets, zDim] and multi-step uses optional recursive roll-forward.

ONLINE LEARNING ALGORITHM (READOUT)

Default training MUST be allocation-free and online:

* Use RLS (Recursive Least Squares) to update Wout each sample with forgetting factor rlsLambda, inverse-covariance P matrix, and initialization scale rlsDelta.
* Include L2 regularization via l2Lambda on readout weights (implemented in a numerically stable way compatible with RLS).
* The readout update is always incremental, with fixed-size matrix updates (no backprop, no Adam).

MULTI-STEP FORECASTING

* Direct multi-horizon head (preferred default): outputDim = nTargets * maxFutureSteps; single state update / forward; slice requested steps.
* Optional recursive roll-forward: generate one step at a time using a separate scratch rollforward buffer (no allocations) and NEVER corrupt the main history buffer.
* Recursive roll-forward MUST be explicitly configured and MUST NOT implicitly feed predictions into the RingBuffer.

SCOPE OF “AUTOMATIC LAGS”

* The model learns effective lags implicitly via reservoir fading memory and nonlinear state dynamics.
* Enforce maxSequenceLength as a HARD cap for any window assembly and optional state warmup; truncate history rather than allocate larger buffers.
* Reservoir memory length is controlled by leakRate, spectralRadius, and (optional) input scaling; document these relationships.

PERFORMANCE PRINCIPLES (HARD REQUIREMENTS)

* Fixed-size memory: preallocate all parameter, state, covariance (P) for RLS, buffers, and scratch once at initialization; never grow during training.
* No attention matrices and no convolutions: do not implement self-attention or Conv1D; all temporal mixing is via reservoir recurrence with fixed buffers.
* Typed-array only core: all tensors live in contiguous Float64Array slabs in row-major layout; expose zero-copy views via (data, offset, shape, strides).
* No hot-path allocations: updateReservoirState(), forward(), fitOnline(), predict() MUST allocate 0 arrays/objects per call; all temporaries come from TensorArena and BufferPool.
* Strict reuse policy: rent scratch buffers by size class, return immediately; no new Float64Array inside inner loops.
* In-place and fused kernels: prefer fused ops (matvec+bias+activation, leaky update, readout matvec, RLS update steps) and in-place transforms to avoid intermediates.
* Deterministic caps: enforce maxSequenceLength and maxBatch=1; if inputs exceed limits, truncate/stride rather than allocating more memory.
* Minimal training tape: no full backprop tape; store only what RLS needs (current z_t, scalar denom, gain vector, and small stats).
* Tight loops only: classic for-loops with manual indexing; avoid map/reduce/forEach, closures, iterator protocols, and polymorphic dispatch in inner loops.
* Cache once, reuse forever: initialization masks, sparsity patterns, scale factors, and shape metadata computed once and reused; lazy init allowed, but never re-init.
* GC guardrails: pool small objects (TensorView, ForwardContext shells); keep debug and verbose metrics off by default.

DESIGN PRINCIPLES (HARD REQUIREMENTS)

* Full numerical stability throughout (epsilon guards, finite checks, variance floors, RLS stabilization).
* Object-oriented design with interfaces for public contracts; private field encapsulation for internal state.
* JSDoc on public methods and key classes (@param, @returns, @example).
* Inline math formula notes where it clarifies behavior (normalization, loss, RLS, reservoir update, spectral radius scaling).
* NO full backpropagation through time and NO training of reservoir weights; only trainable component is the readout Wout.

PUBLIC API (MUST MATCH EXACTLY)

fitOnline({ xCoordinates: number[][], yCoordinates: number[][] }): FitResult

* Validate strictly: xCoordinates.length === yCoordinates.length. If mismatch, throw Error.
* For i in 0..N-1, the order is mandatory and must never change:

  1. Push xCoordinates[i] into RingBuffer FIRST.
  2. Then run: normalize, updateReservoirState (state warmup uses history window ending at latest X), forwardTrain, loss, outlier weight, L2/readout regularization, readoutUpdate (RLS), residual update.
* Reuse a single FitResult object (no per-call allocations).

predict(futureSteps: number): PredictionResult (with confidence based on loss)

* Uses ONLY internal RingBuffer to assemble the latest input window (ending at the most recent ingested X).
* Validate: 1 <= futureSteps <= maxFutureSteps.
* Direct multi-horizon (default): compute current reservoir state consistently (including normalization) and do single readout; slice requested steps.
* Recursive (optional): uses separate scratch rollforward window and scratch state copy; never mutates RingBuffer or live reservoir state.
* Denormalize outputs and return uncertainty bounds from ResidualStatsTracker.
* MUST be deterministic and allocation-free in hot path.

getModelSummary(): ModelSummary
getWeights(): WeightInfo
getNormalizationStats(): NormalizationStats
reset(): void
save(): string  (JSON.stringify of all state)
load(w: string): void  (restore all state)

RESULT TYPES (MUST MATCH EXACTLY)

/**

* Result of fitOnline() call.
  */
  export interface FitResult {
  /** Number of samples processed */
  samplesProcessed: number;
  /** Average loss over processed samples */
  averageLoss: number;
  /** Final readout update norm */
  gradientNorm: number;
  /** Whether concept drift was detected */
  driftDetected: boolean;
  /** Current sample weight (outlier-adjusted) */
  sampleWeight: number;
  }

Note: driftDetected MUST remain in FitResult for API compatibility; since ADWIN is removed, set driftDetected=false always (unless you implement an alternative drift heuristic without extra allocations).

/**

* Result of predict() call.
  */
  export interface PredictionResult {
  /** Predictions for each future step: [step][target] */
  predictions: number[][];
  /** Lower confidence bounds: [step][target] */
  lowerBounds: number[][];
  /** Upper confidence bounds: [step][target] */
  upperBounds: number[][];
  /** Confidence score based on recent loss */
  confidence: number;
  }

/**

* Model summary information.
  */
  export interface ModelSummary {
  /** Total number of trainable parameters */
  totalParameters: number;
  /** Effective memory horizon in timesteps (approx from config) */
  receptiveField: number;
  /** Reservoir spectral radius */
  spectralRadius: number;
  /** Reservoir size */
  reservoirSize: number;
  /** Input features */
  nFeatures: number; //automatic by x[0].length
  /** Output targets */
  nTargets: number; //automatic by y[0].length
  /** Maximum sequence length */
  maxSequenceLength: number;
  /** Maximum future steps */
  maxFutureSteps: number;
  /** Training samples seen */
  sampleCount: number;
  /** Whether direct multi-horizon is enabled */
  useDirectMultiHorizon: boolean;
  }

/**

* Weight information for inspection.
  */
  export interface WeightInfo {
  /** Named weight tensors */
  weights: Array<{ name: string; shape: number[]; values: number[] }>;
  }

/**

* Normalization statistics.
  */
  export interface NormalizationStats {
  /** Feature-wise means */
  means: number[];
  /** Feature-wise standard deviations */
  stds: number[];
  /** Number of samples used */
  count: number;
  /** Whether normalization is active (warmup complete) */
  isActive: boolean;
  }

SETTINGS WITH DEFAULT VALUES (MUST MATCH)

ESNRegressionConfig {
maxSequenceLength: number              // Default: 64
maxFutureSteps: number                 // Default: 1
reservoirSize: number                  // Default: 256
spectralRadius: number                 // Default: 0.9
leakRate: number                       // Default: 0.3
inputScale: number                     // Default: 1.0
biasScale: number                       // Default: 0.1
reservoirSparsity: number              // Default: 0.9      // fraction of zeros in W
inputSparsity: number                  // Default: 0.0      // optional sparsity for Win
activation: "tanh" | "relu"            // Default: "tanh"
useInputInReadout: boolean             // Default: true
useBiasInReadout: boolean              // Default: true
readoutTraining: "rls"                 // Default: "rls"
rlsLambda: number                      // Default: 0.999
rlsDelta: number                       // Default: 1.0
epsilon: number                        // Default: 1e-8
l2Lambda: number                       // Default: 0.0001
gradientClipNorm: number               // Default: 1.0      // applies to readout updates
normalizationEpsilon: number           // Default: 1e-8
normalizationWarmup: number            // Default: 10
outlierThreshold: number               // Default: 3.0
outlierMinWeight: number               // Default: 0.1
useDirectMultiHorizon: boolean         // Default: true
residualWindowSize: number             // Default: 100
uncertaintyMultiplier: number          // Default: 1.96
weightInitScale: number                // Default: 0.1
seed: number                           // Default: 42
verbose: boolean                       // Default: false
}

INPUT EXAMPLE:
const xTrain = [
  [1.0, 2.0], //t
  [1.5, 2.5], //t+1
  [2.0, 3.0], //t+2
  [2.5, 3.5], //t+3
  [3.0, 4.0], //t+4
];

const yTrain = [
  [10, 11, 12], //t
  [15, 16, 17], //t+1
  [20, 21, 22], //t+2
  [25, 26, 27], //t+3
  [30, 31, 32], //t+4
];

CLASSES (MUST IMPLEMENT EXACTLY AS LISTED)
Implement the following classes exactly as listed, using typed arrays, fixed-size preallocation, and pooled shell objects as required:
TensorShape, TensorView, TensorArena, BufferPool, TensorOps, ActivationOps, RandomGenerator,
WelfordAccumulator, WelfordNormalizer, LayerNormParams, LayerNormOps,
GradientAccumulator,
ReservoirInitMask, SpectralRadiusScaler, ESNReservoirParams, ESNReservoir,
ReadoutConfig, ReadoutParams, LinearReadout, RLSState, RLSOptimizer,
LinearLayerParams, LinearLayer, DropoutMask,
ForwardContext, BackwardContext, GradientTape,
RingBuffer, ResidualStatsTracker,
OutlierDownweighter, LossFunction, MetricsAccumulator,
ESNModelConfig, ESNModel, TrainingState, InferenceState,
FitResult, PredictionResult, ModelSummary, WeightInfo, NormalizationStats,
SerializationHelper, ESNRegression.

IMPLEMENTATION ORDER (KEEP FOCUSED AND FAST)

1. Memory infra: TensorShape, TensorView, BufferPool, TensorArena, TensorOps.
2. Numerics: RNG, activations, Welford normalizers, loss.
3. Readout training: RLSState + RLSOptimizer (online, fixed-size P matrix).
4. Reservoir: init masks, sparse W/Win init, spectral radius scaling, ESNReservoir update kernel.
5. Readout/head: LinearReadout for direct multi-horizon, slicing logic, optional recursive roll-forward with scratch state.
6. Training utilities: RingBuffer, ResidualStatsTracker, outinglier downweighting, metrics.
7. ESNModel assembly and parameter registry (trainable readout only).
8. ESNRegression public API enforcing the Critical Behavioral Requirements (especially: ingest X first, correct predict window).
9. Serialization save/load for full state and determinism.
