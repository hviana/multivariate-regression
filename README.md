# ğŸ§  ESNRegression

<div align="center">
**Self-contained TypeScript Echo State Network (ESN) / Reservoir Computing library for online multivariate regression**

_Created by **Henrique Emanoel Viana**_

</div>

---

## ğŸ“‹ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸš€ Installation](#-installation)
- [âš¡ Quick Start](#-quick-start)
- [ğŸ“ Understanding Echo State Networks](#-understanding-echo-state-networks)
- [ğŸ”§ Configuration Parameters](#-configuration-parameters)
- [ğŸ“– API Reference](#-api-reference)
- [ğŸ’¡ Examples & Use Cases](#-examples--use-cases)
- [ğŸ¯ Parameter Optimization Guide](#-parameter-optimization-guide)
- [ğŸ“Š Performance Tips](#-performance-tips)
- [ğŸ“œ License](#-license)

---

## âœ¨ Features

<div align="center">

| Feature                           | Description                                                       |
| --------------------------------- | ----------------------------------------------------------------- |
| ğŸ”„ **Online Learning**            | Real-time incremental learning with RLS (Recursive Least Squares) |
| ğŸ“ˆ **Multivariate Regression**    | Handle multiple input features and output targets simultaneously  |
| ğŸ”® **Multi-Horizon Prediction**   | Forecast multiple steps into the future with confidence intervals |
| ğŸ¯ **Outlier Robust**             | Automatic outlier detection and downweighting                     |
| ğŸ“Š **Adaptive Normalization**     | Welford's online algorithm for streaming statistics               |
| ğŸ”’ **Deterministic**              | Reproducible results with seeded random number generation         |
| âš¡ **Zero Dependencies**          | Self-contained implementation with no external libraries          |
| ğŸ§® **Memory Efficient**           | Pre-allocated tensor arena with minimal garbage collection        |
| ğŸ’¾ **Serialization**              | Full save/load support for model persistence                      |
| ğŸ“ **Uncertainty Quantification** | Prediction intervals with configurable confidence levels          |

</div>

### ğŸŒŸ Key Highlights

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                         â”‚
â”‚  âš¡ REAL-TIME          ğŸ¯ ACCURATE           ğŸ“Š INTERPRETABLE           â”‚
â”‚     Processing            Predictions            Results                â”‚
â”‚                                                                         â”‚
â”‚  â€¢ Stream data          â€¢ Multi-horizon        â€¢ Confidence bounds      â”‚
â”‚  â€¢ No batching            forecasting          â€¢ Residual tracking      â”‚
â”‚  â€¢ Instant updates      â€¢ Autoregressive       â€¢ Weight inspection      â”‚
â”‚                           rollout                                       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Installation

### Deno / JSR

```typescript
import { ESNRegression } from "jsr:@hviana/multivariate-regression";
```

### NPM (via JSR)

```bash
npx jsr add @hviana/multivariate-regression
```

```typescript
import { ESNRegression } from "@hviana/multivariate-regression";
```

---

## âš¡ Quick Start

```typescript
import { ESNRegression } from "jsr:@hviana/multivariate-regression";

// ğŸ”¨ Create model with configuration
const model = new ESNRegression({
  reservoirSize: 256,
  maxSequenceLength: 64,
  spectralRadius: 0.9,
  leakRate: 0.3,
});

// ğŸ“¥ Prepare training data
const xCoordinates = [
  [1.0, 2.0, 3.0], // Features at t=0
  [1.1, 2.1, 3.1], // Features at t=1
  [1.2, 2.2, 3.2], // Features at t=2
  // ... more samples
];

const yCoordinates = [
  [4.0, 5.0], // Targets at t=0
  [4.1, 5.1], // Targets at t=1
  [4.2, 5.2], // Targets at t=2
  // ... more samples
];

// ğŸ¯ Train the model (online, incremental)
const fitResult = model.fitOnline({ xCoordinates, yCoordinates });

console.log(`ğŸ“Š Samples processed: ${fitResult.samplesProcessed}`);
console.log(`ğŸ“‰ Average loss: ${fitResult.averageLoss.toFixed(6)}`);

// ğŸ”® Predict future values
const predictions = model.predict(10); // Predict 10 steps ahead

console.log("ğŸ”® Predictions:", predictions.predictions);
console.log("ğŸ“Š Confidence:", predictions.confidence);
console.log("ğŸ“‰ Lower bounds:", predictions.lowerBounds);
console.log("ğŸ“ˆ Upper bounds:", predictions.upperBounds);
```

---

## ğŸ“ Understanding Echo State Networks

### ğŸ§  What is an Echo State Network?

An **Echo State Network (ESN)** is a type of recurrent neural network that
belongs to the **Reservoir Computing** paradigm. The key innovation is that only
the **output weights are trained**, while the internal reservoir weights remain
fixed after initialization.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚                    ECHO STATE NETWORK ARCHITECTURE                          â”‚
â”‚                                                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚    â”‚         â”‚      â”‚           RESERVOIR                â”‚     â”‚         â”‚  â”‚
â”‚    â”‚  INPUT  â”‚â”€â”€â”€â”€â–¶â”‚    â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”      â”‚â”€â”€â”€â–¶â”‚ OUTPUT  â”‚  â”‚
â”‚    â”‚   x(t)  â”‚ Win  â”‚    â”‚ Nâ‚â”œâ”€â”€â”¤ Nâ‚‚â”œâ”€â”€â”¤ Nâ‚ƒâ”œâ”€â”€â”¤ Nâ‚„â”‚      â”‚Wout â”‚  y(t)   â”‚  â”‚
â”‚    â”‚         â”‚      â”‚    â””â”€â”¬â”€â”˜  â””â”€â”¬â”€â”˜  â””â”€â”¬â”€â”˜  â””â”€â”¬â”€â”˜      â”‚     â”‚         â”‚  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚      â”‚      â”‚      â”‚      â”‚        â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚      â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜        â”‚                  â”‚
â”‚                     â”‚           Recurrent W              â”‚                  â”‚
â”‚                     â”‚         (Fixed weights)            â”‚                  â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                             â”‚
â”‚    Legend:                                                                  â”‚
â”‚    â•â•â•â•â•â•â•                                                                  â”‚
â”‚    Win  = Input weights (fixed after init)                                  â”‚
â”‚    W    = Reservoir weights (fixed after init)                              â”‚
â”‚    Wout = Output weights (TRAINED via RLS)                                  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚                         ESN PROCESSING PIPELINE                             â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚ Raw Data â”‚â”€â”€â–¶â”‚ Normalize  â”‚â”€â”€â–¶â”‚  Reservoir â”‚â”€â”€â–¶â”‚ Build State  â”‚      â”‚
â”‚   â”‚  x_raw   â”‚    â”‚   x_norm   â”‚    â”‚   Update   â”‚    â”‚   Vector z   â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                              â”‚              â”‚
â”‚                                                              â–¼              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚  Output  â”‚â—€â”€â”€â”‚   Linear   â”‚â—€â”€â”€â”‚  Weighted  â”‚â—€â”€â”€â”‚   Concat:    â”‚      â”‚
â”‚   â”‚   y_hat  â”‚    â”‚   Readout  â”‚    â”‚     RLS    â”‚    â”‚ [r, x, bias] â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“ Mathematical Foundation

#### Reservoir State Update (Leaky Integration)

The reservoir state evolves according to:

```
r(t) = (1 - Î±) Â· r(t-1) + Î± Â· f(Win Â· (s Â· x(t)) + W Â· r(t-1) + b)
```

Where:

- **r(t)** = Current reservoir state
- **Î±** = Leak rate (temporal smoothing)
- **f** = Activation function (tanh or ReLU)
- **Win** = Input weight matrix
- **s** = Input scale factor
- **W** = Reservoir weight matrix
- **b** = Bias vector

#### Output Computation

```
z(t) = [r(t), x(t), 1]  (concatenation)
y(t) = Wout Â· z(t)
```

#### Recursive Least Squares (RLS) Update

```
k = PÂ·z / (Î» + z'Â·PÂ·z)
Wout = Wout + kÂ·(y_true - y_hat)'
P = (P - kÂ·z'Â·P) / Î»
```

### ğŸŒ€ Why Reservoir Computing Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚                    THE ECHO STATE PROPERTY                                  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚                                                                  â”‚       â”‚
â”‚  â”‚  When spectral radius < 1, the reservoir has "fading memory":    â”‚       â”‚
â”‚  â”‚                                                                  â”‚       â”‚
â”‚  â”‚  â€¢ Past inputs influence decays exponentially over time          â”‚       â”‚
â”‚  â”‚  â€¢ Network state is uniquely determined by input history         â”‚       â”‚
â”‚  â”‚  â€¢ No exploding/vanishing gradient problems                      â”‚       â”‚
â”‚  â”‚                                                                  â”‚       â”‚
â”‚  â”‚            Memory Decay                                          â”‚       â”‚
â”‚  â”‚        â–²                                                         â”‚       â”‚
â”‚  â”‚        â”‚  â–ˆâ–ˆâ–ˆâ–ˆ                                                   â”‚       â”‚
â”‚  â”‚        â”‚  â–ˆâ–ˆâ–ˆâ–ˆ â–“â–“â–“â–“                                              â”‚       â”‚
â”‚  â”‚        â”‚  â–ˆâ–ˆâ–ˆâ–ˆ â–“â–“â–“â–“ â–‘â–‘â–‘â–‘                                         â”‚       â”‚
â”‚  â”‚        â”‚  â–ˆâ–ˆâ–ˆâ–ˆ â–“â–“â–“â–“ â–‘â–‘â–‘â–‘ Â·Â·Â·Â·                                    â”‚       â”‚
â”‚  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Time                            â”‚       â”‚
â”‚  â”‚           t-3   t-2   t-1   t                                    â”‚       â”‚
â”‚  â”‚                                                                  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Configuration Parameters

### ğŸ“Š Complete Configuration Reference

```typescript
interface ESNRegressionConfig {
  // ğŸ”„ Reservoir Architecture
  maxSequenceLength: number; // Default: 64
  reservoirSize: number; // Default: 256
  spectralRadius: number; // Default: 0.9
  leakRate: number; // Default: 0.3
  inputScale: number; // Default: 1.0
  biasScale: number; // Default: 0.1
  reservoirSparsity: number; // Default: 0.9
  inputSparsity: number; // Default: 0.0
  activation: "tanh" | "relu"; // Default: "tanh"

  // ğŸ“¤ Readout Configuration
  useInputInReadout: boolean; // Default: true
  useBiasInReadout: boolean; // Default: true

  // ğŸ¯ Training (RLS)
  readoutTraining: "rls"; // Default: "rls"
  rlsLambda: number; // Default: 0.999
  rlsDelta: number; // Default: 1.0
  epsilon: number; // Default: 1e-8
  l2Lambda: number; // Default: 0.0001
  gradientClipNorm: number; // Default: 1.0

  // ğŸ“Š Normalization
  normalizationEpsilon: number; // Default: 1e-8
  normalizationWarmup: number; // Default: 10

  // ğŸ›¡ï¸ Outlier Handling
  outlierThreshold: number; // Default: 3.0
  outlierMinWeight: number; // Default: 0.1

  // ğŸ“ˆ Uncertainty
  residualWindowSize: number; // Default: 100
  uncertaintyMultiplier: number; // Default: 1.96

  // âš™ï¸ Initialization
  weightInitScale: number; // Default: 0.1
  seed: number; // Default: 42
  verbose: boolean; // Default: false
  rollforwardMode: "holdLastX" | "autoregressive"; // Default: "holdLastX"
}
```

---

### ğŸ”„ Reservoir Architecture Parameters

#### `reservoirSize` ğŸ¯

**What it does:** Determines the number of neurons in the reservoir (hidden
layer).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RESERVOIR SIZE IMPACT                                  â”‚
â”‚                                                                             â”‚
â”‚   Size: 64              Size: 256             Size: 1024                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚ â€¢ â€¢ â€¢ â”‚             â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚         â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚           â”‚
â”‚   â”‚ â€¢ â€¢ â€¢ â”‚             â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚         â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚           â”‚
â”‚   â”‚ â€¢ â€¢ â€¢ â”‚             â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚         â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚         â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚           â”‚
â”‚   Fast, limited         â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚         â”‚ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â”‚           â”‚
â”‚   expressiveness        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                         Balanced              High capacity,                â”‚
â”‚                                               slower training               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Use Case              | Recommended Size | Rationale            |
| --------------------- | ---------------- | -------------------- |
| Simple linear trends  | 32-64            | Low complexity, fast |
| Standard time series  | 128-256          | Good balance         |
| Complex patterns      | 512-1024         | High capacity needed |
| Multi-variate complex | 256-512          | Per-target capacity  |

**Example:**

```typescript
// ğŸš€ For simple univariate prediction
const simpleModel = new ESNRegression({
  reservoirSize: 64,
});

// ğŸ¯ For complex multivariate forecasting
const complexModel = new ESNRegression({
  reservoirSize: 512,
});
```

---

#### `spectralRadius` ğŸ“Š

**What it does:** Controls the "memory" of the network. It's the largest
eigenvalue of the reservoir weight matrix.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SPECTRAL RADIUS EFFECT                                  â”‚
â”‚                                                                             â”‚
â”‚  Memory Retention                                                           â”‚
â”‚       â–²                                                                     â”‚
â”‚       â”‚                                                                     â”‚
â”‚  1.0 â”€â”¤                               â”Œâ”€â”€â”€â”€â”€ Ï = 0.99 (Long memory)         â”‚
â”‚       â”‚                          â”Œâ”€â”€â”€â”€â”˜                                     â”‚
â”‚       â”‚                     â”Œâ”€â”€â”€â”€â”˜                                          â”‚
â”‚  0.5 â”€â”¤                â”Œâ”€â”€â”€â”€â”˜          â”Œâ”€â”€â”€â”€â”€ Ï = 0.9 (Medium memory)       â”‚
â”‚       â”‚           â”Œâ”€â”€â”€â”€â”˜               â”‚                                    â”‚
â”‚       â”‚      â”Œâ”€â”€â”€â”€â”˜              â”Œâ”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚       â”‚ â”Œâ”€â”€â”€â”€â”˜              â”Œâ”€â”€â”€â”€â”˜      â”Œâ”€â”€â”€â”€â”€ Ï = 0.5 (Short memory)       â”‚
â”‚  0.0 â”€â”´â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Time Steps           â”‚
â”‚       0         5         10        15        20                            â”‚
â”‚                                                                             â”‚
â”‚  âš ï¸  Warning: Ï â‰¥ 1.0 can cause instability (loss of echo state property)   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Data Characteristics          | Recommended Ï | Why                               |
| ----------------------------- | ------------- | --------------------------------- |
| Rapid changes, short patterns | 0.5 - 0.7     | Quick adaptation                  |
| Standard time series          | 0.8 - 0.95    | Balanced memory                   |
| Long-term dependencies        | 0.95 - 0.99   | Extended memory                   |
| Near edge of chaos            | 0.99          | Maximum expressiveness (careful!) |

**Example:**

```typescript
// ğŸ“ˆ Stock prices (long memory needed)
const stockModel = new ESNRegression({
  spectralRadius: 0.95,
});

// âš¡ Sensor data (rapid changes)
const sensorModel = new ESNRegression({
  spectralRadius: 0.7,
});
```

---

#### `leakRate` ğŸ’§

**What it does:** Controls temporal smoothing in the reservoir update. Values
closer to 1 mean faster updates; values closer to 0 provide more smoothing.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LEAK RATE DYNAMICS                                  â”‚
â”‚                                                                             â”‚
â”‚  r(t) = (1 - Î±) Â· r(t-1) + Î± Â· f(...)                                       â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚                                                                 â”‚        â”‚
â”‚  â”‚    Î± = 0.1 (Slow leak)     â”‚    Î± = 0.9 (Fast leak)             â”‚        â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚        â”‚
â”‚  â”‚    â”‚   â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“    â”‚    â”‚    â”‚   â–“â–“â–“â–“           â”‚            â”‚        â”‚
â”‚  â”‚    â”‚  â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“   â”‚    â”‚    â”‚  â–“â–“â–“â–“â–“â–“â–“â–“        â”‚            â”‚        â”‚
â”‚  â”‚    â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“  â”‚    â”‚    â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“ â”‚            â”‚        â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚        â”‚
â”‚  â”‚    Smooth, averaged        â”‚    Responsive, reactive            â”‚        â”‚
â”‚  â”‚                            â”‚                                    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Application           | Recommended Î± | Behavior           |
| --------------------- | ------------- | ------------------ |
| Noisy data            | 0.1 - 0.3     | Smoothing effect   |
| Standard forecasting  | 0.3 - 0.5     | Balanced           |
| Fast-changing signals | 0.6 - 0.9     | Quick response     |
| Real-time tracking    | 0.8 - 1.0     | Immediate reaction |

**Example:**

```typescript
// ğŸŒŠ Noisy sensor smoothing
const smoothModel = new ESNRegression({
  leakRate: 0.2,
});

// âš¡ High-frequency trading
const fastModel = new ESNRegression({
  leakRate: 0.8,
});
```

---

#### `inputScale` ğŸ“

**What it does:** Scales the input before feeding to the reservoir.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INPUT SCALING EFFECT                                 â”‚
â”‚                                                                             â”‚
â”‚   Input Scale = 0.1          Input Scale = 1.0          Input Scale = 3.0   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚    Â·Â·Â·Â·Â·       â”‚         â”‚   â•±â•²           â”‚         â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚
â”‚   â”‚   Â·Â·Â·Â·Â·        â”‚         â”‚  â•±  â•²          â”‚         â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ”‚  â”‚
â”‚   â”‚    Â·Â·Â·Â·Â·       â”‚         â”‚ â•±    â•²â•±â•²       â”‚         â”‚â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ  â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   Weak influence             Balanced                   Strong, may saturateâ”‚
â”‚   (underutilized)            (recommended)              activation function â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Data Type                 | Recommended Scale | Notes                       |
| ------------------------- | ----------------- | --------------------------- |
| Pre-normalized (-1 to 1)  | 0.5 - 1.0         | Standard range              |
| Large magnitude           | 0.1 - 0.5         | Prevent saturation          |
| Small signals             | 1.0 - 2.0         | Amplify for better dynamics |
| With online normalization | 1.0               | Let normalizer handle it    |

---

#### `biasScale` âš–ï¸

**What it does:** Scales the random bias values in the reservoir.

```typescript
// Typical configurations
const model = new ESNRegression({
  biasScale: 0.1, // Default - small bias contribution
});

// For breaking symmetry in sparse reservoirs
const sparseModel = new ESNRegression({
  reservoirSparsity: 0.95,
  biasScale: 0.2, // Slightly larger to add diversity
});
```

---

#### `reservoirSparsity` ğŸ•¸ï¸

**What it does:** Controls the proportion of zero connections in the reservoir
matrix.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       RESERVOIR SPARSITY                                    â”‚
â”‚                                                                             â”‚
â”‚   Sparsity = 0.0 (Dense)      Sparsity = 0.9 (90% zeros)                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚      â”‚ Â·  Â·  â–ˆ  Â·  Â·  Â· â”‚                          â”‚
â”‚   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚      â”‚ Â·  â–ˆ  Â·  Â·  â–ˆ  Â· â”‚                          â”‚
â”‚   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚      â”‚ â–ˆ  Â·  Â·  Â·  Â·  â–ˆ â”‚                          â”‚
â”‚   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚      â”‚ Â·  Â·  â–ˆ  Â·  Â·  Â· â”‚                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚   Slow, potentially           Fast, biologically                            â”‚
â”‚   overfit                     plausible                                     â”‚
â”‚                                                                             â”‚
â”‚   ğŸ¯ Recommended: 0.8 - 0.95 for most applications                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example:**

```typescript
// Standard sparse reservoir (recommended)
const model = new ESNRegression({
  reservoirSparsity: 0.9, // 90% zeros, 10% connections
});

// Dense reservoir (more capacity, slower)
const denseModel = new ESNRegression({
  reservoirSparsity: 0.5,
});
```

---

#### `inputSparsity` ğŸ“¥

**What it does:** Controls sparsity of input-to-reservoir connections.

| Setting     | Use Case                       |
| ----------- | ------------------------------ |
| 0.0 (dense) | All features equally important |
| 0.3 - 0.5   | Feature selection effect       |
| 0.7 - 0.9   | Very high-dimensional inputs   |

---

#### `activation` âš¡

**What it does:** Non-linear activation function for reservoir neurons.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       ACTIVATION FUNCTIONS                                  â”‚
â”‚                                                                             â”‚
â”‚      tanh                              relu                                 â”‚
â”‚       â–²                                 â–²                                   â”‚
â”‚   1.0â”€â”¤      â•­â”€â”€â”€â”€â”€â”€â”€â”€              1.0â”€â”¤           â•±â•±â•±â•±                    â”‚
â”‚       â”‚    â•­â”€â•¯                          â”‚         â•±â•±                        â”‚
â”‚   0.0â”€â”¼â”€â”€â”€â”€â•¯â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              0.0â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚       â”‚â”€â”€â•®                              â”‚â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚  -1.0â”€â”¤  â•°â”€â”€â”€â”€â”€â”€â”€â”€                 -1.0â”€â”¤                                   â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶                    â”‚
â”‚                                                                             â”‚
â”‚   â€¢ Bounded (-1, 1)              â€¢ Unbounded (0, âˆ)                         â”‚
â”‚   â€¢ Smoother gradients           â€¢ Sparse activations                       â”‚
â”‚   â€¢ âœ… Default choice            â€¢ Good for positive data                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example:**

```typescript
// Standard (recommended for most cases)
const tanhModel = new ESNRegression({
  activation: "tanh",
});

// For positive-only predictions
const reluModel = new ESNRegression({
  activation: "relu",
});
```

---

#### `maxSequenceLength` ğŸ“

**What it does:** Maximum temporal context window and prediction horizon limit.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SEQUENCE LENGTH CONTEXT                                 â”‚
â”‚                                                                             â”‚
â”‚   maxSequenceLength = 64                                                    â”‚
â”‚                                                                             â”‚
â”‚   â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ History Buffer (Ring Buffer) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”             â”‚
â”‚   â”‚ tâ‚€ â”‚ tâ‚ â”‚ tâ‚‚ â”‚ tâ‚ƒ â”‚ .. â”‚tâ‚†â‚â”‚tâ‚†â‚‚â”‚tâ‚†â‚ƒâ”‚ ğŸ”® â”‚ ğŸ”® â”‚ ğŸ”® â”‚ .. â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜             â”‚
â”‚   â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Stored Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚â—€â”€â”€ predict(N) â”€â”€â”€â”€â–¶             â”‚
â”‚                                                                             â”‚
â”‚   âš ï¸  predict(futureSteps) must be â‰¤ maxSequenceLength                      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Scenario            | Recommended Length | Notes                   |
| ------------------- | ------------------ | ----------------------- |
| Real-time streaming | 32-64              | Low latency             |
| Daily forecasting   | 64-128             | ~2 months of daily data |
| Long-term patterns  | 128-512            | Seasonal effects        |

---

### ğŸ“¤ Readout Configuration

#### `useInputInReadout` ğŸ“

**What it does:** When `true`, appends current input to reservoir state for
output computation.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      READOUT STATE COMPOSITION                              â”‚
â”‚                                                                             â”‚
â”‚   useInputInReadout: true    â”‚    useInputInReadout: false                  â”‚
â”‚   useBiasInReadout: true     â”‚    useBiasInReadout: false                   â”‚
â”‚                              â”‚                                              â”‚
â”‚   z = [râ‚,râ‚‚,...,râ‚™, xâ‚,xâ‚‚,xâ‚˜, 1]    z = [râ‚,râ‚‚,...,râ‚™]                     â”‚
â”‚       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”¬â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚        reservoir      input   bias        reservoir                         â”‚
â”‚         state                  only                                         â”‚
â”‚                                                                             â”‚
â”‚   âœ… Better for:             â”‚    âœ… Better for:                            â”‚
â”‚   â€¢ Direct input influence   â”‚    â€¢ Pure temporal features                  â”‚
â”‚   â€¢ Skip connections         â”‚    â€¢ Minimal state size                      â”‚
â”‚                              â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Default recommendation:** Keep both `true` for most applications.

---

### ğŸ¯ Training Parameters (RLS)

#### `rlsLambda` Î»

**What it does:** Forgetting factor for Recursive Least Squares. Controls how
quickly old information is discarded.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       RLS FORGETTING FACTOR                                 â”‚
â”‚                                                                             â”‚
â”‚   Î» = 0.99 (Slow forget)         Î» = 0.95 (Fast forget)                     â”‚
â”‚                                                                             â”‚
â”‚   Weight on past data:           Weight on past data:                       â”‚
â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             â”‚
â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             â”‚
â”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              â”‚
â”‚      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               â”‚
â”‚       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                â”‚
â”‚   â—€â”€â”€â”€ Past â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶           â—€â”€â”€â”€ Past â”€â”€â”€â”€â–¶                        â”‚
â”‚                                                                             â”‚
â”‚   â€¢ Stable learning              â€¢ Adaptive to changes                      â”‚
â”‚   â€¢ Good for stationary data     â€¢ Good for non-stationary data             â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Data Behavior   | Î» Value        | Why                      |
| --------------- | -------------- | ------------------------ |
| Stationary      | 0.999 - 0.9999 | Stable, uses all history |
| Slowly drifting | 0.995 - 0.999  | Balanced                 |
| Concept drift   | 0.95 - 0.99    | Quick adaptation         |
| Rapid changes   | 0.9 - 0.95     | Very responsive          |

**Example:**

```typescript
// Stable environment
const stableModel = new ESNRegression({
  rlsLambda: 0.999,
});

// Non-stationary data with drift
const adaptiveModel = new ESNRegression({
  rlsLambda: 0.97,
});
```

---

#### `rlsDelta` Î´

**What it does:** Initial value for the diagonal of the P matrix (inverse
covariance). Larger values = faster initial learning.

```typescript
// Quick initial convergence
const quickStart = new ESNRegression({
  rlsDelta: 10.0,
});

// Conservative start
const conservativeStart = new ESNRegression({
  rlsDelta: 0.1,
});
```

| Setting       | Effect                             |
| ------------- | ---------------------------------- |
| 0.01 - 0.1    | Slow, conservative initial updates |
| 1.0 (default) | Balanced                           |
| 10.0 - 100.0  | Aggressive initial learning        |

---

#### `l2Lambda` ğŸ›¡ï¸

**What it does:** L2 regularization (weight decay) applied to readout weights.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        L2 REGULARIZATION EFFECT                             â”‚
â”‚                                                                             â”‚
â”‚   No Regularization (l2Lambda = 0)    With Regularization (l2Lambda > 0)    â”‚
â”‚                                                                             â”‚
â”‚   Weight magnitudes:                  Weight magnitudes:                    â”‚
â”‚   â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“                     â–“â–“â–“â–“â–“â–“                                 â”‚
â”‚       â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“                     â–“â–“â–“â–“â–“                              â”‚
â”‚   â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“                 â–“â–“â–“â–“â–“â–“â–“                                â”‚
â”‚                                                                             â”‚
â”‚   â€¢ May overfit                      â€¢ Prevents overfitting                 â”‚
â”‚   â€¢ Potentially unstable             â€¢ More stable                          â”‚
â”‚   â€¢ Large weight swings              â€¢ Smoother predictions                 â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Data Size            | Recommended l2Lambda | Notes                 |
| -------------------- | -------------------- | --------------------- |
| Small (<100 samples) | 0.01 - 0.1           | Strong regularization |
| Medium (100-1000)    | 0.0001 - 0.001       | Moderate              |
| Large (>1000)        | 0.00001 - 0.0001     | Light regularization  |

---

#### `gradientClipNorm` âœ‚ï¸

**What it does:** Clips the update norm to prevent explosive updates.

```typescript
// Standard (default)
const model = new ESNRegression({
  gradientClipNorm: 1.0,
});

// More aggressive clipping for unstable data
const safeModel = new ESNRegression({
  gradientClipNorm: 0.5,
});

// Disabled (not recommended)
const unclippedModel = new ESNRegression({
  gradientClipNorm: 0, // No clipping
});
```

---

### ğŸ“Š Normalization Parameters

#### `normalizationWarmup` ğŸ”¥

**What it does:** Number of samples before online normalization becomes active.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      NORMALIZATION WARMUP                                  â”‚
â”‚                                                                            â”‚
â”‚   Samples:  1   2   3   4   5   6   7   8   9  10  11  12 ...              â”‚
â”‚            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚            Warmup Phase     â”‚    Normal Operation                          â”‚
â”‚            (collecting      â”‚    (active normalization)                    â”‚
â”‚             statistics)     â”‚                                              â”‚
â”‚                             â”‚                                              â”‚
â”‚   normalizationWarmup = 10 â”€â”˜                                              â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ›¡ï¸ Outlier Handling Parameters

#### `outlierThreshold` ğŸ¯

**What it does:** Z-score threshold above which samples are considered outliers.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       OUTLIER DETECTION                                     â”‚
â”‚                                                                             â”‚
â”‚   Residual Distribution                                                     â”‚
â”‚                                                                             â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚                       â•±â”‚       â”‚â•²                                           â”‚
â”‚                      â•± â”‚       â”‚ â•²                                          â”‚
â”‚                     â•±  â”‚       â”‚  â•²                                         â”‚
â”‚                    â•±   â”‚       â”‚   â•²                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚            â—€â”€3Ïƒâ”€â–¶â”‚â—€â”€â”€Normalâ”€â”€â–¶â”‚â—€â”€3Ïƒâ”€â–¶                                 â”‚
â”‚                  â”‚            â”‚                                             â”‚
â”‚            Outlier Zone  â”‚    Outlier Zone                                  â”‚
â”‚                                                                             â”‚
â”‚   outlierThreshold = 3.0 â†’ Samples beyond 3Ïƒ are downweighted               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Setting       | Detection Rate   | Use Case              |
| ------------- | ---------------- | --------------------- |
| 2.0           | ~5% outliers     | Aggressive filtering  |
| 3.0 (default) | ~0.3% outliers   | Standard              |
| 4.0           | ~0.006% outliers | Only extreme outliers |

---

#### `outlierMinWeight` âš–ï¸

**What it does:** Minimum weight assigned to detected outliers (prevents
complete exclusion).

```typescript
// Standard - outliers still contribute minimally
const model = new ESNRegression({
  outlierThreshold: 3.0,
  outlierMinWeight: 0.1, // 10% weight for outliers
});

// Zero tolerance - completely ignore extreme outliers
const strictModel = new ESNRegression({
  outlierThreshold: 2.5,
  outlierMinWeight: 0.0, // Full exclusion
});
```

---

### ğŸ“ˆ Uncertainty Quantification

#### `residualWindowSize` ğŸ“Š

**What it does:** Number of recent residuals used to estimate prediction
uncertainty.

```typescript
// Short window - reacts quickly to error changes
const reactiveModel = new ESNRegression({
  residualWindowSize: 50,
});

// Long window - stable uncertainty estimates
const stableModel = new ESNRegression({
  residualWindowSize: 200,
});
```

---

#### `uncertaintyMultiplier` ğŸ“

**What it does:** Multiplier for confidence interval width (default 1.96 â‰ˆ 95%
CI).

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   UNCERTAINTY MULTIPLIER (Gaussian)                         â”‚
â”‚                                                                             â”‚
â”‚   Multiplier â”‚ Confidence Level â”‚ Interpretation                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚     1.00     â”‚      68.3%       â”‚ Within 1 std deviation                    â”‚
â”‚     1.64     â”‚      90.0%       â”‚ Common for forecasting                    â”‚
â”‚     1.96     â”‚      95.0%       â”‚ Standard (default)                        â”‚
â”‚     2.58     â”‚      99.0%       â”‚ High confidence                           â”‚
â”‚     3.00     â”‚      99.7%       â”‚ Very conservative                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example:**

```typescript
// 90% confidence intervals
const model90 = new ESNRegression({
  uncertaintyMultiplier: 1.64,
});

// 99% confidence intervals (wider bands)
const model99 = new ESNRegression({
  uncertaintyMultiplier: 2.58,
});
```

---

### âš™ï¸ Initialization & Control

#### `seed` ğŸŒ±

**What it does:** Random seed for deterministic weight initialization.

```typescript
// Same seed = identical results
const model1 = new ESNRegression({ seed: 42 });
const model2 = new ESNRegression({ seed: 42 });
// model1 and model2 will produce identical results

// Different seeds for ensemble diversity
const ensemble = [
  new ESNRegression({ seed: 1 }),
  new ESNRegression({ seed: 2 }),
  new ESNRegression({ seed: 3 }),
];
```

---

#### `rollforwardMode` ğŸ”„

**What it does:** Determines how multi-step predictions are generated.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PREDICTION ROLLFORWARD MODES                            â”‚
â”‚                                                                             â”‚
â”‚   "holdLastX" (Default)           â”‚   "autoregressive"                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚
â”‚                                   â”‚                                         â”‚
â”‚   x_known â”€â”¬â”€â–¶ Å·â‚                â”‚   x_known â”€â”¬â”€â–¶ Å·â‚ â”€â”€â”                  â”‚
â”‚            â”‚                      â”‚            â”‚        â”‚                   â”‚
â”‚   x_known â”€â”¼â”€â–¶ Å·â‚‚                â”‚            â””â”€â–¶ Å·â‚‚ â”€â”€â”¤ (Å·â‚ as x)        â”‚
â”‚            â”‚                      â”‚               â”‚     â”‚                   â”‚
â”‚   x_known â”€â”´â”€â–¶ Å·â‚ƒ                â”‚               â””â”€â–¶ Å·â‚ƒ (Å·â‚‚ as x)         â”‚
â”‚                                   â”‚                                         â”‚
â”‚   âœ… Safe, no error              â”‚   âœ… True multi-step                     â”‚
â”‚      accumulation                â”‚      (requires nFeatures == nTargets)    â”‚
â”‚                                   â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example:**

```typescript
// Standard forecasting (safer)
const holdModel = new ESNRegression({
  rollforwardMode: "holdLastX",
});

// Autoregressive (when features = targets)
const arModel = new ESNRegression({
  rollforwardMode: "autoregressive",
});
```

---

## ğŸ“– API Reference

### ğŸ”¨ Constructor

```typescript
constructor(config?: Partial<ESNRegressionConfig>)
```

Creates a new ESNRegression instance with optional configuration overrides.

---

### ğŸ“¥ `fitOnline()`

```typescript
fitOnline(args: { 
  xCoordinates: number[][]; 
  yCoordinates: number[][] 
}): FitResult
```

Incrementally trains the model with new samples.

```typescript
interface FitResult {
  samplesProcessed: number; // Number of samples in this batch
  averageLoss: number; // Running average MSE
  gradientNorm: number; // L2 norm of last weight update
  driftDetected: boolean; // Reserved for drift detection
  sampleWeight: number; // Weight of last sample (outlier handling)
}
```

**Example:**

```typescript
// Single sample update
const result = model.fitOnline({
  xCoordinates: [[1.0, 2.0]],
  yCoordinates: [[3.0]],
});

// Batch update
const batchResult = model.fitOnline({
  xCoordinates: [
    [1.0, 2.0],
    [1.1, 2.1],
    [1.2, 2.2],
  ],
  yCoordinates: [
    [3.0],
    [3.1],
    [3.2],
  ],
});
```

---

### ğŸ”® `predict()`

```typescript
predict(futureSteps: number): PredictionResult
```

Generates multi-horizon predictions with uncertainty bounds.

```typescript
interface PredictionResult {
  predictions: number[][]; // [futureSteps][nTargets]
  lowerBounds: number[][]; // Lower confidence bounds
  upperBounds: number[][]; // Upper confidence bounds
  confidence: number; // Overall confidence (0-1)
}
```

**Example:**

```typescript
const predictions = model.predict(5);

for (let step = 0; step < predictions.predictions.length; step++) {
  console.log(`Step ${step + 1}:`);
  console.log(`  Prediction: ${predictions.predictions[step]}`);
  console.log(
    `  95% CI: [${predictions.lowerBounds[step]}, ${
      predictions.upperBounds[step]
    }]`,
  );
}
console.log(
  `Overall confidence: ${(predictions.confidence * 100).toFixed(1)}%`,
);
```

---

### ğŸ“Š `getModelSummary()`

```typescript
getModelSummary(): ModelSummary
```

Returns model architecture and training statistics.

```typescript
interface ModelSummary {
  totalParameters: number;
  receptiveField: number;
  spectralRadius: number;
  reservoirSize: number;
  nFeatures: number;
  nTargets: number;
  maxSequenceLength: number;
  sampleCount: number;
}
```

---

### âš–ï¸ `getWeights()`

```typescript
getWeights(): WeightInfo
```

Returns all model weights for inspection or custom analysis.

```typescript
interface WeightInfo {
  weights: Array<{
    name: string; // "Win", "W", "b", "Wout", "P"
    shape: number[]; // Dimensions
    values: number[]; // Flattened values
  }>;
}
```

---

### ğŸ“ˆ `getNormalizationStats()`

```typescript
getNormalizationStats(): NormalizationStats
```

Returns current normalization statistics.

```typescript
interface NormalizationStats {
  means: number[]; // Running means per feature
  stds: number[]; // Running standard deviations
  count: number; // Samples seen
  isActive: boolean; // Whether warmup is complete
}
```

---

### ğŸ”„ `reset()`

```typescript
reset(): void
```

Resets model to initial state while preserving configuration.

---

### ğŸ’¾ `save()` / `load()`

```typescript
save(): string
load(serialized: string): void
```

Serializes/deserializes the complete model state.

**Example:**

```typescript
// Save model
const modelState = model.save();
localStorage.setItem("myModel", modelState);

// Load model
const loadedModel = new ESNRegression();
loadedModel.load(localStorage.getItem("myModel")!);
```

---

## ğŸ’¡ Examples & Use Cases

### ğŸ“ˆ Time Series Forecasting

```typescript
import { ESNRegression } from "jsr:@hviana/multivariate-regression";

// Configuration for daily sales forecasting
const salesModel = new ESNRegression({
  reservoirSize: 256,
  maxSequenceLength: 90, // 3 months of history
  spectralRadius: 0.95, // Long-term patterns
  leakRate: 0.3, // Smooth transitions
  rlsLambda: 0.998, // Slow forgetting
  uncertaintyMultiplier: 1.96, // 95% CI
});

// Train with historical data
const historicalSales = [
  { features: [100, 5, 1], target: [120] }, // [base_sales, promo, weekday] -> [actual]
  { features: [110, 0, 2], target: [105] },
  // ... more data
];

for (const sample of historicalSales) {
  salesModel.fitOnline({
    xCoordinates: [sample.features],
    yCoordinates: [sample.target],
  });
}

// Forecast next 7 days
const forecast = salesModel.predict(7);

console.log("ğŸ“Š 7-Day Sales Forecast:");
forecast.predictions.forEach((pred, day) => {
  console.log(
    `  Day ${day + 1}: ${pred[0].toFixed(0)} ` +
      `[${forecast.lowerBounds[day][0].toFixed(0)} - ${
        forecast.upperBounds[day][0].toFixed(0)
      }]`,
  );
});
```

---

### ğŸ¤– Online Sensor Fusion

```typescript
// Real-time sensor data processing
const sensorModel = new ESNRegression({
  reservoirSize: 128,
  maxSequenceLength: 32,
  leakRate: 0.7, // Fast response
  spectralRadius: 0.8, // Short memory
  rlsLambda: 0.95, // Quick adaptation
  outlierThreshold: 2.5, // Aggressive outlier rejection
  activation: "tanh",
});

// Streaming sensor loop
async function processSensorStream(sensorStream: AsyncIterable<SensorReading>) {
  for await (const reading of sensorStream) {
    // Input: [temperature, humidity, pressure, light]
    // Output: [predicted_occupancy, energy_demand]

    const result = sensorModel.fitOnline({
      xCoordinates: [[
        reading.temp,
        reading.humidity,
        reading.pressure,
        reading.light,
      ]],
      yCoordinates: [[reading.occupancy, reading.energy]],
    });

    if (result.sampleWeight < 0.5) {
      console.warn("âš ï¸ Potential sensor anomaly detected!");
    }

    // Get 1-step ahead prediction for real-time control
    const prediction = sensorModel.predict(1);

    await sendToController({
      predictedOccupancy: prediction.predictions[0][0],
      predictedEnergy: prediction.predictions[0][1],
      confidence: prediction.confidence,
    });
  }
}
```

---

### ğŸ“Š Multivariate Financial Prediction

```typescript
// Multi-asset price prediction
const financeModel = new ESNRegression({
  reservoirSize: 512, // High capacity
  maxSequenceLength: 128, // ~6 months daily data
  spectralRadius: 0.99, // Long memory (markets have trends)
  leakRate: 0.2, // Smooth (noisy data)
  inputSparsity: 0.3, // Feature selection
  rlsLambda: 0.995,
  l2Lambda: 0.001, // Regularization
  rollforwardMode: "autoregressive", // True multi-step
  uncertaintyMultiplier: 2.58, // 99% CI for risk management
});

// Input: [asset1_return, asset2_return, volatility_index, interest_rate]
// Output: [asset1_next, asset2_next] (same features for autoregressive)

const trainingData = prepareFinancialData();

// Batch training
financeModel.fitOnline({
  xCoordinates: trainingData.x,
  yCoordinates: trainingData.y,
});

// 5-day forecast
const forecast = financeModel.predict(5);

console.log("ğŸ“ˆ 5-Day Multi-Asset Forecast:");
console.log(`Confidence: ${(forecast.confidence * 100).toFixed(1)}%`);
forecast.predictions.forEach((pred, day) => {
  console.log(
    `  Day ${day + 1}: Asset1=${pred[0].toFixed(4)}, Asset2=${
      pred[1].toFixed(4)
    }`,
  );
});
```

---

### ğŸ”„ Model Persistence & Deployment

```typescript
// Training phase
const model = new ESNRegression({ reservoirSize: 256 });

// ... train model ...

// Save for deployment
const modelState = model.save();
await Deno.writeTextFile("model.json", modelState);

// -----------------------------------

// Deployment / Loading
const deployedModel = new ESNRegression();
const savedState = await Deno.readTextFile("model.json");
deployedModel.load(savedState);

// Continue training (transfer learning)
deployedModel.fitOnline({
  xCoordinates: newData.x,
  yCoordinates: newData.y,
});
```

---

## ğŸ¯ Parameter Optimization Guide

### ğŸ—ºï¸ Decision Flowchart

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                            â”‚
â”‚                    PARAMETER SELECTION GUIDE                               â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                     START HERE                                      â”‚   â”‚
â”‚  â”‚                          â”‚                                          â”‚   â”‚
â”‚  â”‚                          â–¼                                          â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚   â”‚
â”‚  â”‚    â”‚     What is your data volume?           â”‚                      â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚   â”‚
â”‚  â”‚                    â”‚                                                â”‚   â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚   â”‚
â”‚  â”‚         â–¼          â–¼          â–¼                                     â”‚   â”‚
â”‚  â”‚      Small      Medium      Large                                   â”‚   â”‚
â”‚  â”‚    (<1000)    (1K-100K)    (>100K)                                  â”‚   â”‚
â”‚  â”‚         â”‚          â”‚          â”‚                                     â”‚   â”‚
â”‚  â”‚         â–¼          â–¼          â–¼                                     â”‚   â”‚
â”‚  â”‚    reservoirSize  reservoirSize  reservoirSize                      â”‚   â”‚
â”‚  â”‚      64-128       128-512      256-1024                             â”‚   â”‚
â”‚  â”‚    l2Lambda      l2Lambda     l2Lambda                              â”‚   â”‚
â”‚  â”‚     0.01          0.001       0.0001                                â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚                          â”‚                                          â”‚   â”‚
â”‚  â”‚                          â–¼                                          â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚   â”‚
â”‚  â”‚    â”‚     Is your data stationary?            â”‚                      â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚   â”‚
â”‚  â”‚                    â”‚                                                â”‚   â”‚
â”‚  â”‚              â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                          â”‚   â”‚
â”‚  â”‚              â–¼           â–¼                                          â”‚   â”‚
â”‚  â”‚            Yes          No                                          â”‚   â”‚
â”‚  â”‚              â”‚           â”‚                                          â”‚   â”‚
â”‚  â”‚              â–¼           â–¼                                          â”‚   â”‚
â”‚  â”‚        rlsLambda    rlsLambda                                       â”‚   â”‚
â”‚  â”‚         0.999        0.95-0.99                                      â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚                          â”‚                                          â”‚   â”‚
â”‚  â”‚                          â–¼                                          â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚   â”‚
â”‚  â”‚    â”‚     Pattern length in your data?        â”‚                      â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚   â”‚
â”‚  â”‚                    â”‚                                                â”‚   â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚   â”‚
â”‚  â”‚         â–¼          â–¼          â–¼                                     â”‚   â”‚
â”‚  â”‚       Short     Medium      Long                                    â”‚   â”‚
â”‚  â”‚      (<10)     (10-50)     (>50)                                    â”‚   â”‚
â”‚  â”‚         â”‚          â”‚          â”‚                                     â”‚   â”‚
â”‚  â”‚         â–¼          â–¼          â–¼                                     â”‚   â”‚
â”‚  â”‚   spectralRadius  spectralRadius  spectralRadius                    â”‚   â”‚
â”‚  â”‚      0.5-0.7      0.8-0.9       0.95-0.99                           â”‚   â”‚
â”‚  â”‚   leakRate       leakRate      leakRate                             â”‚   â”‚
â”‚  â”‚      0.6-0.9      0.3-0.6      0.1-0.3                              â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“‹ Quick Reference Presets

#### ğŸš€ Fast & Simple

```typescript
const quickModel = new ESNRegression({
  reservoirSize: 64,
  maxSequenceLength: 32,
  spectralRadius: 0.8,
  leakRate: 0.5,
  reservoirSparsity: 0.9,
});
```

#### âš–ï¸ Balanced (Default-like)

```typescript
const balancedModel = new ESNRegression({
  reservoirSize: 256,
  maxSequenceLength: 64,
  spectralRadius: 0.9,
  leakRate: 0.3,
  rlsLambda: 0.999,
});
```

#### ğŸ¯ High Accuracy

```typescript
const accurateModel = new ESNRegression({
  reservoirSize: 512,
  maxSequenceLength: 128,
  spectralRadius: 0.95,
  leakRate: 0.2,
  rlsLambda: 0.9995,
  l2Lambda: 0.0001,
});
```

#### ğŸ”„ Adaptive (Non-Stationary)

```typescript
const adaptiveModel = new ESNRegression({
  reservoirSize: 256,
  maxSequenceLength: 64,
  spectralRadius: 0.85,
  leakRate: 0.5,
  rlsLambda: 0.97,
  outlierThreshold: 2.5,
});
```

#### ğŸ›¡ï¸ Robust (Noisy Data)

```typescript
const robustModel = new ESNRegression({
  reservoirSize: 256,
  maxSequenceLength: 64,
  spectralRadius: 0.9,
  leakRate: 0.2, // More smoothing
  outlierThreshold: 2.0, // Stricter
  outlierMinWeight: 0.05,
  l2Lambda: 0.01, // Strong regularization
});
```

---

## ğŸ“Š Performance Tips

### âš¡ Speed Optimization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚                    PERFORMANCE OPTIMIZATION                                 â”‚
â”‚                                                                             â”‚
â”‚  1. RESERVOIR SIZE - Primary cost factor                                    â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚     Memory:  O(NÂ²)     Computation: O(NÂ² + NÃ—F)                             â”‚
â”‚                                                                             â”‚
â”‚     Tip: Start small (64-128), increase only if needed                      â”‚
â”‚                                                                             â”‚
â”‚  2. SPARSITY - Reduce effective computations                                â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚     reservoirSparsity: 0.9  â†’  10% of weights active                        â”‚
â”‚     inputSparsity: 0.5      â†’  50% of inputs connected                      â”‚
â”‚                                                                             â”‚
â”‚  3. BATCH SIZE - Amortize overhead                                          â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚     Single samples: Higher overhead                                         â”‚
â”‚     Batches of 10-100: Better throughput                                    â”‚
â”‚                                                                             â”‚
â”‚  4. PRE-ALLOCATION - Arena already handles this âœ…                          â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚     No GC pressure from model internals                                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¯ Accuracy Tips

1. **Feature Engineering**: Normalize inputs before feeding to model
2. **Proper Warmup**: Allow `normalizationWarmup` samples before expecting good
   predictions
3. **Hyperparameter Tuning**: Use validation set to tune `spectralRadius`,
   `leakRate`
4. **Ensemble Methods**: Create multiple models with different seeds, average
   predictions

```typescript
// Simple ensemble
const ensemble = [
  new ESNRegression({ seed: 1, reservoirSize: 256 }),
  new ESNRegression({ seed: 2, reservoirSize: 256 }),
  new ESNRegression({ seed: 3, reservoirSize: 256 }),
];

function ensemblePredict(models: ESNRegression[], steps: number) {
  const predictions = models.map((m) => m.predict(steps));

  // Average predictions
  return predictions[0].predictions.map((_, stepIdx) =>
    predictions[0].predictions[stepIdx].map((_, targetIdx) => {
      const sum = predictions.reduce(
        (acc, p) => acc + p.predictions[stepIdx][targetIdx],
        0,
      );
      return sum / predictions.length;
    })
  );
}
```

---

## ğŸ§ª Testing Your Configuration

```typescript
import { ESNRegression } from "jsr:@hviana/multivariate-regression";

function evaluateConfig(
  config: Partial<ESNRegressionConfig>,
  data: { x: number[][]; y: number[][] },
) {
  const model = new ESNRegression(config);

  // Split data
  const trainSize = Math.floor(data.x.length * 0.8);
  const trainX = data.x.slice(0, trainSize);
  const trainY = data.y.slice(0, trainSize);
  const testX = data.x.slice(trainSize);
  const testY = data.y.slice(trainSize);

  // Train
  model.fitOnline({ xCoordinates: trainX, yCoordinates: trainY });

  // Evaluate
  let mse = 0;
  for (let i = 0; i < testX.length; i++) {
    model.fitOnline({ xCoordinates: [testX[i]], yCoordinates: [testY[i]] });
    const pred = model.predict(1);

    for (let t = 0; t < testY[i].length; t++) {
      mse += Math.pow(pred.predictions[0][t] - testY[i][t], 2);
    }
  }

  mse /= testX.length * testY[0].length;

  return {
    mse,
    rmse: Math.sqrt(mse),
    summary: model.getModelSummary(),
  };
}

// Test different configurations
const configs = [
  { name: "Small", config: { reservoirSize: 64 } },
  { name: "Medium", config: { reservoirSize: 256 } },
  { name: "Large", config: { reservoirSize: 512 } },
];

for (const { name, config } of configs) {
  const result = evaluateConfig(config, myData);
  console.log(`${name}: RMSE = ${result.rmse.toFixed(4)}`);
}
```

---

## ğŸ“š Additional Resources

### ğŸ“– Learn More About ESNs

- [Scholarpedia: Echo State Network](http://www.scholarpedia.org/article/Echo_state_network)
- [A Practical Guide to ESNs](http://www.faculty.jacobs-university.de/hjaeger/pubs/ESNTutorialRev.pdf)

### ğŸ”— Related Projects

- [JSR Package](https://jsr.io/@hviana/multivariate-regression)
- [GitHub Repository](https://github.com/hviana/multivariate-regression)

---

## ğŸ“œ License

**MIT License** Â© 2025 Henrique Emanoel Viana

```
Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

<div align="center">

**Made with â¤ï¸ by [Henrique Emanoel Viana](https://github.com/hviana)**

â­ Star this repo if you find it useful!

</div>
