Model: # ğŸ§  ESNRegression

<div align="center">

**Echo State Network for Multivariate Autoregressive Regression**

_with RLS Online Learning & Welford Normalization_

[ğŸ“¦ JSR Package](https://jsr.io/@hviana/multivariate-regression) â€¢
[ğŸ“‚ GitHub](https://github.com/hviana/multivariate-regression) â€¢
[ğŸ‘¤ Author: Henrique Emanoel Viana](https://github.com/hviana)

</div>

---

## ğŸ“‘ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ—ï¸ Architecture Overview](#ï¸-architecture-overview)
- [âš™ï¸ Configuration Parameters](#ï¸-configuration-parameters)
  - [ğŸ¯ Reservoir Parameters](#-reservoir-parameters)
  - [ğŸ“Š Training Parameters](#-training-parameters)
  - [ğŸ›¡ï¸ Robustness Parameters](#ï¸-robustness-parameters)
  - [ğŸ”§ Utility Parameters](#-utility-parameters)
- [ğŸ“– API Reference](#-api-reference)
- [ğŸ“ Use Case Examples](#-use-case-examples)
- [ğŸ”¬ Parameter Optimization Guide](#-parameter-optimization-guide)
- [ğŸ’¾ Serialization](#-serialization)
- [ğŸ“„ License](#-license)

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ”„ Online Learning

- **Recursive Least Squares (RLS)** algorithm
- Single-pass training without storing data
- Continuous adaptation to new patterns
- Memory-efficient for streaming data

</td>
<td width="50%">

### ğŸŒ€ Echo State Network

- **Reservoir Computing** paradigm
- Sparse, randomly initialized reservoir
- Spectral radius control for dynamics
- Leaky integrator neurons

</td>
</tr>
<tr>
<td width="50%">

### ğŸ“ˆ Multivariate Support

- Handle **multiple correlated time series**
- Joint prediction of all features
- Cross-feature dependencies captured
- Autoregressive roll-forward prediction

</td>
<td width="50%">

### ğŸ›¡ï¸ Robustness Features

- **Welford online normalization**
- Outlier detection & downweighting
- Uncertainty quantification
- Confidence intervals

</td>
</tr>
</table>

---

## ğŸš€ Quick Start

### Installation

```typescript
import { ESNRegression } from "https://esm.sh/jsr/@hviana/multivariate-regression";
```

### Basic Usage

```typescript
// 1ï¸âƒ£ Create model with default configuration
const model = new ESNRegression();

// 2ï¸âƒ£ Prepare your time series data
const coordinates = [
  [1.0, 2.0, 3.0], // t=0: [feature1, feature2, feature3]
  [1.5, 2.5, 3.5], // t=1
  [2.0, 3.0, 4.0], // t=2
  [2.5, 3.5, 4.5], // t=3
  [3.0, 4.0, 5.0], // t=4
  // ... more data points
];

// 3ï¸âƒ£ Train the model (online learning)
const fitResult = model.fitOnline({ coordinates });
console.log(`ğŸ“Š Average Loss: ${fitResult.averageLoss}`);

// 4ï¸âƒ£ Predict future values
const prediction = model.predict(5); // Predict 5 steps ahead

console.log("ğŸ”® Predictions:", prediction.predictions);
console.log("ğŸ“‰ Lower Bounds:", prediction.lowerBounds);
console.log("ğŸ“ˆ Upper Bounds:", prediction.upperBounds);
console.log("ğŸ¯ Confidence:", prediction.confidence);
```

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ESNRegression Architecture                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   Input x(t)    â”‚
                              â”‚  [n_features]   â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      ğŸ“Š Welford Normalizer           â”‚
                    â”‚  â€¢ Online mean/std computation       â”‚
                    â”‚  â€¢ Warmup period handling            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                        ğŸŒ€ ESN Reservoir                            â”‚
     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
     â”‚  â”‚                                                             â”‚   â”‚
     â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
     â”‚  â”‚    â”‚  W_in   â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚                         â”‚           â”‚   â”‚ 
     â”‚  â”‚    â”‚(input)  â”‚        â”‚    Reservoir State      â”‚           â”‚   â”‚
     â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚      h(t) âˆˆ â„^N         â”‚           â”‚   â”‚
     â”‚  â”‚                       â”‚                         â”‚           â”‚   â”‚
     â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   h(t) = (1-Î±)h(t-1)    â”‚           â”‚   â”‚
     â”‚  â”‚    â”‚    W    â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   + Î±Â·f(W_inÂ·x + WÂ·h    â”‚           â”‚   â”‚
     â”‚  â”‚    â”‚(recur.) â”‚        â”‚        + bias)          â”‚           â”‚   â”‚
     â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚                         â”‚           â”‚   â”‚
     â”‚  â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
     â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚                         â”‚   â”‚
     â”‚  â”‚    â”‚  bias   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚   â”‚
     â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚   â”‚
     â”‚  â”‚                                                             â”‚   â”‚
     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
     â”‚      Spectral Radius: Ï(W) < 1   â”‚   Sparsity Control              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      ğŸ“ Extended State z(t)          â”‚
                    â”‚  z = [h(t), x(t), 1]                 â”‚
                    â”‚  (reservoir + input + bias)          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                      ğŸ“ˆ Linear Readout                              â”‚
     â”‚                                                                     â”‚
     â”‚              y(t) = W_out Â· z(t)                                    â”‚
     â”‚                                                                     â”‚
     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
     â”‚   â”‚              ğŸ¯ RLS Optimizer (Online)                      â”‚   â”‚
     â”‚   â”‚  â€¢ Recursive weight updates                                 â”‚   â”‚
     â”‚   â”‚  â€¢ Forgetting factor (Î») for adaptation                     â”‚   â”‚
     â”‚   â”‚  â€¢ L2 regularization                                        â”‚   â”‚
     â”‚   â”‚  â€¢ Outlier downweighting                                    â”‚   â”‚
     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   Output Å·(t)   â”‚
                              â”‚  [n_features]   â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“Š Data Flow During Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Training Data Flow                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  coordinates[i]          coordinates[i+1]
       â”‚                        â”‚
       â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
       â””â”€â”€â”€â–¶â”‚    INPUT      â”‚   â”‚
            â”‚   x(t) = [i]  â”‚   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                    â”‚           â”‚
                    â–¼           â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
            â”‚   RESERVOIR   â”‚   â”‚
            â”‚   UPDATE      â”‚   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                    â”‚           â”‚
                    â–¼           â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
            â”‚   PREDICT     â”‚   â”‚
            â”‚   Å·(t+1)      â”‚â—„â”€â”€â”˜
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    TARGET
                    â”‚           y(t+1) = [i+1]
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   RLS UPDATE  â”‚
            â”‚   W_out       â”‚
            â”‚   minimize    â”‚
            â”‚   ||Å· - y||Â²  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”® Prediction (Roll-Forward)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Autoregressive Prediction                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Latest State                                    
       â”‚                                          
       â–¼                                          
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
   â”‚ x(T)  â”‚â”€â”€â–¶â”‚Å·(T+1) â”‚â”€â”€â–¶â”‚Å·(T+2) â”‚â”€â”€â–¶â”‚Å·(T+3) â”‚ â”€ â”€ â”€â–¶ ...
   â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚            â”‚            â”‚
                    â–¼            â–¼            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     Uncertainty grows with âˆšstep    â”‚
              â”‚     Ïƒ(step) = Ïƒ_residual Ã— âˆšstep    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Configuration Parameters

### Complete Configuration Interface

```typescript
interface ESNRegressionConfig {
  // ğŸŒ€ Reservoir Parameters
  reservoirSize: number; // Default: 256
  spectralRadius: number; // Default: 0.9
  leakRate: number; // Default: 0.3
  inputScale: number; // Default: 1.0
  biasScale: number; // Default: 0.1
  reservoirSparsity: number; // Default: 0.9
  inputSparsity: number; // Default: 0.0
  activation: "tanh" | "relu"; // Default: "tanh"

  // ğŸ“ Readout Parameters
  useInputInReadout: boolean; // Default: true
  useBiasInReadout: boolean; // Default: true

  // ğŸ“ˆ Training Parameters
  readoutTraining: "rls"; // Default: "rls"
  rlsLambda: number; // Default: 0.999
  rlsDelta: number; // Default: 1.0
  l2Lambda: number; // Default: 0.0001

  // ğŸ›¡ï¸ Robustness Parameters
  normalizationEpsilon: number; // Default: 1e-8
  normalizationWarmup: number; // Default: 10
  outlierThreshold: number; // Default: 3.0
  outlierMinWeight: number; // Default: 0.1
  uncertaintyMultiplier: number; // Default: 1.96

  // ğŸ”§ Utility Parameters
  epsilon: number; // Default: 1e-8
  gradientClipNorm: number; // Default: 1.0
  weightInitScale: number; // Default: 0.1
  seed: number; // Default: 42
  verbose: boolean; // Default: false
}
```

---

### ğŸ¯ Reservoir Parameters

<details>
<summary><b>ğŸ“¦ reservoirSize</b> â€” Size of the reservoir (number of neurons)</summary>

```typescript
reservoirSize: number; // Default: 256
```

**What it does:** Determines the dimensionality of the hidden state space.

**Impact:**

| Value            | Effect                               |
| ---------------- | ------------------------------------ |
| Small (32-128)   | Faster computation, limited capacity |
| Medium (256-512) | Good balance for most tasks          |
| Large (1024+)    | More expressive, risk of overfitting |

**Optimization Guide:**

```typescript
// ğŸ”¹ Simple patterns (seasonal, linear trends)
const simpleModel = new ESNRegression({ reservoirSize: 64 });

// ğŸ”¹ Moderate complexity (stock prices, weather)
const moderateModel = new ESNRegression({ reservoirSize: 256 });

// ğŸ”¹ Complex patterns (high-frequency data, many features)
const complexModel = new ESNRegression({ reservoirSize: 512 });

// ğŸ”¹ Rule of thumb: ~10-50x the number of input features
const nFeatures = 10;
const adaptiveModel = new ESNRegression({
  reservoirSize: Math.max(64, nFeatures * 25),
});
```

</details>

<details>
<summary><b>ğŸŒŠ spectralRadius</b> â€” Controls memory and dynamics stability</summary>

```typescript
spectralRadius: number; // Default: 0.9, Range: (0, 1]
```

**What it does:** Scales the reservoir weight matrix to control how information
echoes through the network.

```
spectralRadius â†’ 0: Shorter memory, faster forgetting
spectralRadius â†’ 1: Longer memory, edge of chaos
spectralRadius > 1: Unstable (avoid!)
```

**Visual Guide:**

```
Memory Capacity vs Spectral Radius:

  Memory â”‚
         â”‚                    â—
         â”‚                 â—
         â”‚              â—
         â”‚          â—
         â”‚      â—
         â”‚  â—
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
            0.5  0.7  0.9  0.99   Ï
```

**Optimization Guide:**

```typescript
// ğŸ”¹ Short-term patterns (high-frequency trading, sensor data)
const shortMemory = new ESNRegression({ spectralRadius: 0.5 });

// ğŸ”¹ Medium-term patterns (daily patterns, typical time series)
const mediumMemory = new ESNRegression({ spectralRadius: 0.9 });

// ğŸ”¹ Long-term dependencies (monthly cycles, slow dynamics)
const longMemory = new ESNRegression({ spectralRadius: 0.99 });

// ğŸ”¹ Chaotic systems (need edge of chaos dynamics)
const chaoticSystem = new ESNRegression({
  spectralRadius: 0.95,
  leakRate: 0.1, // Combine with low leak rate
});
```

</details>

<details>
<summary><b>ğŸ’§ leakRate</b> â€” Neuron integration speed</summary>

```typescript
leakRate: number; // Default: 0.3, Range: (0, 1]
```

**What it does:** Controls how fast neurons update their state (leaky
integrator).

**Formula:**

```
h(t) = (1 - leakRate) Ã— h(t-1) + leakRate Ã— f(input)
```

**Effect Diagram:**

```
leakRate = 0.1 (Slow):    leakRate = 0.9 (Fast):
     â”‚                         â”‚
   h â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚     â”Œâ”€
     â”‚/                        â”‚    /â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ t        â””â”€â”€â”€/â”€â”€â–¶ t
     Smooth, slow response     Quick, responsive
```

**Optimization Guide:**

```typescript
// ğŸ”¹ Smooth, slowly changing data
const smoothData = new ESNRegression({ leakRate: 0.1 });

// ğŸ”¹ Balanced (default for most cases)
const balanced = new ESNRegression({ leakRate: 0.3 });

// ğŸ”¹ Rapidly changing data, needs quick response
const rapidData = new ESNRegression({ leakRate: 0.8 });

// ğŸ”¹ Match to your data's sampling rate
// If sampling 100Hz data that changes at ~10Hz:
const matchedRate = new ESNRegression({ leakRate: 0.1 }); // ~10% update
```

</details>

<details>
<summary><b>ğŸ“ inputScale</b> â€” Input weight magnitude</summary>

```typescript
inputScale: number; // Default: 1.0
```

**What it does:** Scales the input-to-reservoir weight matrix.

**Optimization Guide:**

```typescript
// ğŸ”¹ Normalized input (z-score normalized, range ~[-3, 3])
const normalizedInput = new ESNRegression({ inputScale: 1.0 });

// ğŸ”¹ Small input values (range ~[0, 0.1])
const smallInput = new ESNRegression({ inputScale: 5.0 });

// ğŸ”¹ Large input values (range ~[0, 1000])
// Note: Welford normalizer handles this automatically
const largeInput = new ESNRegression({ inputScale: 0.1 });

// ğŸ”¹ Nonlinear activation saturation control
// Higher inputScale â†’ more nonlinear response
const nonlinear = new ESNRegression({
  inputScale: 2.0,
  activation: "tanh", // Will saturate more with larger inputs
});
```

</details>

<details>
<summary><b>âš¡ biasScale</b> â€” Reservoir bias magnitude</summary>

```typescript
biasScale: number; // Default: 0.1
```

**What it does:** Scales the constant bias added to reservoir neurons.

```typescript
// ğŸ”¹ Default (subtle bias)
const defaultBias = new ESNRegression({ biasScale: 0.1 });

// ğŸ”¹ More diverse neuron responses
const diverseBias = new ESNRegression({ biasScale: 0.5 });

// ğŸ”¹ Minimal bias (rely on input/recurrent)
const minimalBias = new ESNRegression({ biasScale: 0.01 });
```

</details>

<details>
<summary><b>ğŸ•¸ï¸ reservoirSparsity</b> â€” Reservoir connection sparsity</summary>

```typescript
reservoirSparsity: number; // Default: 0.9, Range: [0, 1)
```

**What it does:** Fraction of zero connections in reservoir matrix.

```
sparsity = 0.9  â†’  90% zeros, 10% connections
sparsity = 0.0  â†’  0% zeros, fully connected
```

**Benefits of Sparsity:**

- âš¡ Faster computation (sparse matrix operations)
- ğŸ¯ Better generalization
- ğŸ§  Encourages modularity

```typescript
// ğŸ”¹ Default sparse (efficient)
const sparse = new ESNRegression({ reservoirSparsity: 0.9 });

// ğŸ”¹ Dense reservoir (more expressive, slower)
const dense = new ESNRegression({ reservoirSparsity: 0.5 });

// ğŸ”¹ Very sparse (fast, may lose capacity)
const verySparse = new ESNRegression({ reservoirSparsity: 0.99 });
```

</details>

<details>
<summary><b>ğŸ”Œ inputSparsity</b> â€” Input connection sparsity</summary>

```typescript
inputSparsity: number; // Default: 0.0 (fully connected)
```

**What it does:** Fraction of zero connections in input-to-reservoir matrix.

```typescript
// ğŸ”¹ Full input connectivity (default)
const fullInput = new ESNRegression({ inputSparsity: 0.0 });

// ğŸ”¹ Each neuron sees subset of inputs
const sparseInput = new ESNRegression({ inputSparsity: 0.5 });

// ğŸ”¹ Useful when features are independent
const independentFeatures = new ESNRegression({ inputSparsity: 0.7 });
```

</details>

<details>
<summary><b>âš¡ activation</b> â€” Nonlinear activation function</summary>

```typescript
activation: "tanh" | "relu"; // Default: "tanh"
```

**Comparison:**

| Activation | Characteristics          | Best For                   |
| ---------- | ------------------------ | -------------------------- |
| `tanh`     | Bounded [-1, 1], smooth  | General purpose, stability |
| `relu`     | Unbounded [0, âˆ), sparse | Positive-only patterns     |

```typescript
// ğŸ”¹ General purpose (recommended)
const tanhModel = new ESNRegression({ activation: "tanh" });

// ğŸ”¹ Sparse activations, positive patterns
const reluModel = new ESNRegression({ activation: "relu" });
```

</details>

---

### ğŸ“Š Training Parameters

<details>
<summary><b>ğŸ“ˆ rlsLambda</b> â€” RLS forgetting factor</summary>

```typescript
rlsLambda: number; // Default: 0.999, Range: (0, 1]
```

**What it does:** Controls how quickly past observations are "forgotten".

```
Effective window â‰ˆ 1 / (1 - rlsLambda)

Î» = 0.999  â†’  ~1000 samples effective window
Î» = 0.99   â†’  ~100 samples effective window  
Î» = 0.9    â†’  ~10 samples effective window
```

**Adaptation Speed Diagram:**

```
Î» = 0.999 (Slow adaptation):    Î» = 0.9 (Fast adaptation):
Weight                           Weight
   â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚     /                         â”‚    / â†™ Quick
   â”‚    / â†™ Gradual                â”‚   /   response
   â””â”€â”€â”€/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ t            â””â”€â”€/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ t
   Stable, slow learning          Tracks changes fast
```

**Optimization Guide:**

```typescript
// ğŸ”¹ Stationary data (stable patterns)
const stationary = new ESNRegression({ rlsLambda: 0.9999 });

// ğŸ”¹ Default (slight adaptivity)
const balanced = new ESNRegression({ rlsLambda: 0.999 });

// ğŸ”¹ Non-stationary data (drifting patterns)
const drifting = new ESNRegression({ rlsLambda: 0.99 });

// ğŸ”¹ Highly dynamic (concept drift, regime changes)
const dynamic = new ESNRegression({ rlsLambda: 0.95 });

// ğŸ”¹ Match to expected change rate
// If patterns change every ~500 samples:
const matched = new ESNRegression({ rlsLambda: 1 - 1 / 500 }); // 0.998
```

</details>

<details>
<summary><b>ğŸšï¸ rlsDelta</b> â€” RLS initialization parameter</summary>

```typescript
rlsDelta: number; // Default: 1.0
```

**What it does:** Initial value for the inverse covariance matrix diagonal (P =
I/Î´).

```typescript
// ğŸ”¹ Default (balanced initial uncertainty)
const defaultDelta = new ESNRegression({ rlsDelta: 1.0 });

// ğŸ”¹ High initial uncertainty (conservative start)
const conservative = new ESNRegression({ rlsDelta: 0.1 });

// ğŸ”¹ Low initial uncertainty (confident start)
const confident = new ESNRegression({ rlsDelta: 10.0 });
```

</details>

<details>
<summary><b>ğŸ”’ l2Lambda</b> â€” L2 regularization strength</summary>

```typescript
l2Lambda: number; // Default: 0.0001
```

**What it does:** Weight decay to prevent overfitting.

```
Loss = MSE + l2Lambda Ã— ||W_out||Â²
```

**Optimization Guide:**

```typescript
// ğŸ”¹ Minimal regularization (large data, simple patterns)
const minimal = new ESNRegression({ l2Lambda: 0.00001 });

// ğŸ”¹ Default (balanced)
const balanced = new ESNRegression({ l2Lambda: 0.0001 });

// ğŸ”¹ Strong regularization (small data, complex reservoir)
const strong = new ESNRegression({ l2Lambda: 0.01 });

// ğŸ”¹ Aggressive (prevent overfitting at all costs)
const aggressive = new ESNRegression({ l2Lambda: 0.1 });
```

</details>

<details>
<summary><b>ğŸ”— useInputInReadout / useBiasInReadout</b> â€” Extended state configuration</summary>

```typescript
useInputInReadout: boolean; // Default: true
useBiasInReadout: boolean; // Default: true
```

**Extended State Structure:**

```
z = [ reservoir_state , input , 1 ]
         h(t)           x(t)   bias
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”¬â”˜
    reservoirSize    nFeatures   1

if useInputInReadout=false: z = [h(t), 1]
if useBiasInReadout=false:  z = [h(t), x(t)]
if both=false:              z = [h(t)]
```

```typescript
// ğŸ”¹ Full extended state (recommended)
const full = new ESNRegression({
  useInputInReadout: true,
  useBiasInReadout: true,
});

// ğŸ”¹ Reservoir-only (pure ESN)
const pureESN = new ESNRegression({
  useInputInReadout: false,
  useBiasInReadout: false,
});
```

</details>

---

### ğŸ›¡ï¸ Robustness Parameters

<details>
<summary><b>ğŸ“Š normalizationWarmup</b> â€” Samples before normalization activates</summary>

```typescript
normalizationWarmup: number; // Default: 10
```

**What it does:** Minimum samples needed to estimate reliable statistics.

```typescript
// ğŸ”¹ Quick start (small batches)
const quickStart = new ESNRegression({ normalizationWarmup: 5 });

// ğŸ”¹ Default
const balanced = new ESNRegression({ normalizationWarmup: 10 });

// ğŸ”¹ Conservative (noisy initial data)
const conservative = new ESNRegression({ normalizationWarmup: 50 });
```

</details>

<details>
<summary><b>ğŸš¨ outlierThreshold</b> â€” Z-score threshold for outlier detection</summary>

```typescript
outlierThreshold: number; // Default: 3.0
```

**What it does:** Samples with residual z-score > threshold get downweighted.

```
P(|z| > 3) â‰ˆ 0.3%   (3-sigma rule)
P(|z| > 2) â‰ˆ 4.5%   
P(|z| > 4) â‰ˆ 0.006%
```

```typescript
// ğŸ”¹ Aggressive outlier rejection
const aggressive = new ESNRegression({ outlierThreshold: 2.0 });

// ğŸ”¹ Default (3-sigma rule)
const balanced = new ESNRegression({ outlierThreshold: 3.0 });

// ğŸ”¹ Permissive (only extreme outliers)
const permissive = new ESNRegression({ outlierThreshold: 5.0 });

// ğŸ”¹ Heavy-tailed data (expect more outliers)
const heavyTailed = new ESNRegression({
  outlierThreshold: 4.0,
  outlierMinWeight: 0.3, // Still consider them somewhat
});
```

</details>

<details>
<summary><b>âš–ï¸ outlierMinWeight</b> â€” Minimum weight for outliers</summary>

```typescript
outlierMinWeight: number; // Default: 0.1, Range: [0, 1]
```

**What it does:** Floor for outlier sample weights (prevents complete
rejection).

```typescript
// ğŸ”¹ Aggressive rejection (near-zero weight for outliers)
const aggressive = new ESNRegression({ outlierMinWeight: 0.01 });

// ğŸ”¹ Default
const balanced = new ESNRegression({ outlierMinWeight: 0.1 });

// ğŸ”¹ Soft rejection (outliers still contribute)
const soft = new ESNRegression({ outlierMinWeight: 0.5 });
```

</details>

<details>
<summary><b>ğŸ“ uncertaintyMultiplier</b> â€” Confidence interval width</summary>

```typescript
uncertaintyMultiplier: number; // Default: 1.96
```

**What it does:** Multiplier for prediction interval bounds.

```
Bounds = prediction Â± uncertaintyMultiplier Ã— Ïƒ

1.96 â†’ 95% confidence interval
1.645 â†’ 90% confidence interval
2.576 â†’ 99% confidence interval
```

```typescript
// ğŸ”¹ 90% confidence interval
const ci90 = new ESNRegression({ uncertaintyMultiplier: 1.645 });

// ğŸ”¹ 95% confidence interval (default)
const ci95 = new ESNRegression({ uncertaintyMultiplier: 1.96 });

// ğŸ”¹ 99% confidence interval (conservative)
const ci99 = new ESNRegression({ uncertaintyMultiplier: 2.576 });
```

</details>

---

### ğŸ”§ Utility Parameters

<details>
<summary><b>ğŸŒ± seed</b> â€” Random seed for reproducibility</summary>

```typescript
seed: number; // Default: 42
```

**What it does:** Initializes the random number generator for reservoir weights.

```typescript
// ğŸ”¹ Reproducible experiments
const reproducible = new ESNRegression({ seed: 12345 });

// ğŸ”¹ Different random initialization
const model1 = new ESNRegression({ seed: 1 });
const model2 = new ESNRegression({ seed: 2 });
const model3 = new ESNRegression({ seed: 3 });
// Can ensemble these for better predictions
```

</details>

<details>
<summary><b>ğŸ”¢ epsilon / normalizationEpsilon</b> â€” Numerical stability constants</summary>

```typescript
epsilon: number; // Default: 1e-8
normalizationEpsilon: number; // Default: 1e-8
```

**What it does:** Prevents division by zero in numerical operations.

```typescript
// ğŸ”¹ Default (works for most cases)
const standard = new ESNRegression({ epsilon: 1e-8 });

// ğŸ”¹ Higher precision (if seeing numerical issues)
const highPrecision = new ESNRegression({ epsilon: 1e-10 });
```

</details>

<details>
<summary><b>ğŸ“ weightInitScale</b> â€” Output weight initialization scale</summary>

```typescript
weightInitScale: number; // Default: 0.1
```

**What it does:** Standard deviation for initializing readout weights.

```typescript
// ğŸ”¹ Conservative start (near-zero predictions initially)
const conservative = new ESNRegression({ weightInitScale: 0.01 });

// ğŸ”¹ Default
const balanced = new ESNRegression({ weightInitScale: 0.1 });

// ğŸ”¹ Aggressive initialization
const aggressive = new ESNRegression({ weightInitScale: 1.0 });
```

</details>

---

## ğŸ“– API Reference

### Constructor

```typescript
constructor(config?: Partial<ESNRegressionConfig>)
```

Creates a new ESNRegression model with the specified configuration.

---

### Methods

#### `fitOnline(params: { coordinates: number[][] }): FitResult`

Train the model on a sequence of coordinate vectors.

```typescript
interface FitResult {
  samplesProcessed: number; // Number of training pairs processed
  averageLoss: number; // Mean squared error during training
  gradientNorm: number; // Magnitude of parameter updates
  driftDetected: boolean; // Concept drift detection flag
  sampleWeight: number; // Last sample's outlier weight
}
```

**Example:**

```typescript
const data = [
  [1, 2, 3],
  [2, 3, 4],
  [3, 4, 5],
  [4, 5, 6],
];

const result = model.fitOnline({ coordinates: data });
console.log(`Processed ${result.samplesProcessed} samples`);
console.log(`Average loss: ${result.averageLoss.toFixed(6)}`);
```

---

#### `predict(futureSteps: number): PredictionResult`

Generate multi-step ahead predictions with uncertainty bounds.

```typescript
interface PredictionResult {
  predictions: number[][]; // [step][feature] predicted values
  lowerBounds: number[][]; // Lower confidence bounds
  upperBounds: number[][]; // Upper confidence bounds
  confidence: number; // Overall model confidence [0, 1]
}
```

**Example:**

```typescript
const pred = model.predict(10);

for (let step = 0; step < 10; step++) {
  console.log(`Step ${step + 1}:`);
  console.log(`  Prediction: ${pred.predictions[step]}`);
  console.log(
    `  Range: [${pred.lowerBounds[step]}, ${pred.upperBounds[step]}]`,
  );
}
console.log(`Model confidence: ${(pred.confidence * 100).toFixed(1)}%`);
```

---

#### `getModelSummary(): ModelSummary`

Get summary statistics about the model.

```typescript
interface ModelSummary {
  totalParameters: number; // Total learnable parameters
  receptiveField: number; // Effective memory length
  spectralRadius: number; // Current spectral radius
  reservoirSize: number; // Reservoir dimension
  nFeatures: number; // Input/output dimension
  nTargets: number; // Target dimension (same as nFeatures)
  sampleCount: number; // Total samples processed
}
```

---

#### `getWeights(): WeightInfo`

Retrieve all weight matrices for inspection.

```typescript
interface WeightInfo {
  weights: Array<{
    name: string; // "Wout", "Win", "W", "bias"
    shape: number[]; // Matrix dimensions
    values: number[]; // Flattened values
  }>;
}
```

---

#### `getNormalizationStats(): NormalizationStats`

Get current normalization statistics.

```typescript
interface NormalizationStats {
  means: number[]; // Per-feature means
  stds: number[]; // Per-feature standard deviations
  count: number; // Samples used for estimation
  isActive: boolean; // Whether normalization is active
}
```

---

#### `reset(): void`

Reset the model to initial state.

---

#### `save(): string`

Serialize the model to a JSON string.

---

#### `load(str: string): void`

Load model state from a JSON string.

---

## ğŸ“ Use Case Examples

### ğŸ“ˆ Stock Price Prediction

```typescript
import { ESNRegression } from "jsr:@hviana/multivariate-regression";

// Configuration optimized for financial time series
const stockModel = new ESNRegression({
  reservoirSize: 256,
  spectralRadius: 0.95, // Good memory for trends
  leakRate: 0.2, // Smooth integration
  rlsLambda: 0.995, // Adapt to market changes
  outlierThreshold: 2.5, // Financial data has outliers
  outlierMinWeight: 0.2, // Don't completely ignore them
  l2Lambda: 0.001, // Regularization for stability
});

// Data: [open, high, low, close, volume]
const stockData = [
  [150.0, 152.0, 149.0, 151.5, 1000000],
  [151.5, 153.0, 150.0, 152.0, 1100000],
  [152.0, 154.0, 151.0, 153.5, 950000],
  // ... more data
];

// Train
const result = stockModel.fitOnline({ coordinates: stockData });

// Predict next 5 trading days
const forecast = stockModel.predict(5);

console.log("ğŸ“ˆ 5-Day Stock Forecast:");
forecast.predictions.forEach((pred, i) => {
  console.log(
    `Day ${i + 1}: Close = $${pred[3].toFixed(2)} ` +
      `[${forecast.lowerBounds[i][3].toFixed(2)} - ` +
      `${forecast.upperBounds[i][3].toFixed(2)}]`,
  );
});
```

---

### ğŸŒ¡ï¸ Weather Forecasting

```typescript
// Configuration for weather (has daily/seasonal patterns)
const weatherModel = new ESNRegression({
  reservoirSize: 512, // More capacity for complex patterns
  spectralRadius: 0.99, // Long memory for seasonal patterns
  leakRate: 0.1, // Slow dynamics
  rlsLambda: 0.9999, // Weather patterns are stable
  normalizationWarmup: 30, // Need good stats for weather
  seed: 42,
});

// Data: [temperature, humidity, pressure, wind_speed]
const weatherData: number[][] = [
  // ... hourly readings
];

weatherModel.fitOnline({ coordinates: weatherData });

// Predict next 24 hours
const forecast = weatherModel.predict(24);

console.log("ğŸŒ¡ï¸ 24-Hour Weather Forecast:");
forecast.predictions.forEach((pred, hour) => {
  console.log(
    `Hour ${hour + 1}: ` +
      `Temp=${pred[0].toFixed(1)}Â°C, ` +
      `Humidity=${pred[1].toFixed(0)}%`,
  );
});
```

---

### ğŸ¤– Sensor Data / IoT

```typescript
// Configuration for high-frequency sensor data
const sensorModel = new ESNRegression({
  reservoirSize: 128, // Smaller for speed
  spectralRadius: 0.7, // Shorter memory for sensors
  leakRate: 0.5, // Quick response
  rlsLambda: 0.99, // Adapt to sensor drift
  outlierThreshold: 3.5, // Sensors can be noisy
  activation: "relu", // Good for positive-only readings
});

// Real-time training loop
async function processSensorStream() {
  let buffer: number[][] = [];

  for await (const reading of sensorStream) {
    buffer.push(reading);

    if (buffer.length >= 100) {
      // Train on batch
      sensorModel.fitOnline({ coordinates: buffer });

      // Get next prediction for anomaly detection
      const pred = sensorModel.predict(1);

      // Check if current reading is within bounds
      const isAnomaly = reading.some((val, i) =>
        val < pred.lowerBounds[0][i] || val > pred.upperBounds[0][i]
      );

      if (isAnomaly) {
        console.log("âš ï¸ Anomaly detected!", reading);
      }

      buffer = buffer.slice(-50); // Keep recent context
    }
  }
}
```

---

### ğŸ® Motion Prediction

```typescript
// Configuration for smooth trajectory prediction
const motionModel = new ESNRegression({
  reservoirSize: 192,
  spectralRadius: 0.85,
  leakRate: 0.4,
  rlsLambda: 0.99,
  useInputInReadout: true, // Direct path helps smooth predictions
  useBiasInReadout: true,
  uncertaintyMultiplier: 1.96,
});

// Data: [x, y, z, vx, vy, vz] (position + velocity)
const trajectoryData: number[][] = [
  // ... motion capture data
];

motionModel.fitOnline({ coordinates: trajectoryData });

// Predict next 30 frames (1 second at 30fps)
const trajectory = motionModel.predict(30);

// Smooth predictions for animation
trajectory.predictions.forEach((pred, frame) => {
  const [x, y, z, vx, vy, vz] = pred;
  renderFrame({ x, y, z, vx, vy, vz });
});
```

---

### ğŸ“Š Multi-variate Economic Indicators

```typescript
// Configuration for economic time series (monthly data)
const econModel = new ESNRegression({
  reservoirSize: 384,
  spectralRadius: 0.98, // Economic cycles are long
  leakRate: 0.15, // Slow-moving indicators
  rlsLambda: 0.9995, // Very stable patterns
  l2Lambda: 0.0005, // Moderate regularization
  normalizationWarmup: 24, // Need 2 years for good stats
});

// Data: [GDP_growth, unemployment, inflation, interest_rate]
const economicData: number[][] = [
  // ... monthly readings
];

econModel.fitOnline({ coordinates: economicData });

// Forecast next 12 months
const forecast = econModel.predict(12);
const confidence = forecast.confidence;

console.log(
  `ğŸ“Š Economic Forecast (Confidence: ${(confidence * 100).toFixed(1)}%):`,
);
const months = [
  "Jan",
  "Feb",
  "Mar",
  "Apr",
  "May",
  "Jun",
  "Jul",
  "Aug",
  "Sep",
  "Oct",
  "Nov",
  "Dec",
];

forecast.predictions.forEach((pred, i) => {
  console.log(
    `${months[i]}: GDP=${pred[0].toFixed(2)}%, ` +
      `Unemployment=${pred[1].toFixed(1)}%, ` +
      `Inflation=${pred[2].toFixed(2)}%`,
  );
});
```

---

## ğŸ”¬ Parameter Optimization Guide

### Decision Flowchart

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  What type of data do you have? â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚                         â”‚
        â–¼                         â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚High-frequency â”‚       â”‚   Standard    â”‚       â”‚ Low-frequency â”‚
â”‚  (>1Hz)       â”‚       â”‚ (hourly/daily)â”‚       â”‚(weekly/monthly)
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â–¼                       â–¼                       â–¼
 reservoirSize: 128      reservoirSize: 256      reservoirSize: 384
 spectralRadius: 0.7     spectralRadius: 0.9     spectralRadius: 0.98
 leakRate: 0.5-0.8       leakRate: 0.3           leakRate: 0.1-0.2
        â”‚                       â”‚                       â”‚
        â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Is the data stationary?                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                                   â”‚
              â–¼                                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    YES    â”‚                       â”‚    NO     â”‚
        â”‚ Stationaryâ”‚                       â”‚  Drifting â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚                                   â”‚
              â–¼                                   â–¼
       rlsLambda: 0.999-0.9999             rlsLambda: 0.95-0.99
       l2Lambda: 0.0001                    l2Lambda: 0.001
              â”‚                                   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚        How noisy is the data?     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                     â”‚                     â”‚
              â–¼                     â–¼                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Clean   â”‚         â”‚  Moderate â”‚         â”‚   Noisy   â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚                     â”‚                     â”‚
              â–¼                     â–¼                     â–¼
 outlierThreshold: 4.0    outlierThreshold: 3.0   outlierThreshold: 2.5
 outlierMinWeight: 0.01   outlierMinWeight: 0.1   outlierMinWeight: 0.3
```

### Quick Reference Table

| Scenario        | reservoirSize | spectralRadius | leakRate | rlsLambda    | outlierThreshold |
| --------------- | ------------- | -------------- | -------- | ------------ | ---------------- |
| **HFT/Sensors** | 64-128        | 0.5-0.7        | 0.5-0.8  | 0.95-0.99    | 3.0-4.0          |
| **Daily Stock** | 256-384       | 0.9-0.95       | 0.2-0.3  | 0.995-0.999  | 2.5-3.0          |
| **Weather**     | 384-512       | 0.95-0.99      | 0.1-0.2  | 0.999-0.9999 | 3.0-3.5          |
| **Economic**    | 256-384       | 0.95-0.98      | 0.1-0.15 | 0.9995+      | 3.0-4.0          |
| **Motion**      | 128-256       | 0.8-0.9        | 0.3-0.5  | 0.99-0.995   | 3.0-4.0          |

---

## ğŸ’¾ Serialization

### Save and Load Models

```typescript
// Train and save
const model = new ESNRegression({ reservoirSize: 256 });
model.fitOnline({ coordinates: trainingData });

const modelJson = model.save();
// Store to file, database, etc.
Deno.writeTextFileSync("model.json", modelJson);

// Later: load and use
const loadedModel = new ESNRegression();
loadedModel.load(Deno.readTextFileSync("model.json"));

const prediction = loadedModel.predict(5);
```

### Incremental Training

```typescript
// Day 1: Initial training
const model = new ESNRegression();
model.fitOnline({ coordinates: day1Data });
const checkpoint1 = model.save();

// Day 2: Continue training
model.fitOnline({ coordinates: day2Data });
const checkpoint2 = model.save();

// Day 3: Continue training
model.fitOnline({ coordinates: day3Data });

// Rollback to Day 2 if needed
model.load(checkpoint2);
```

---

## ğŸ”§ Troubleshooting

| Problem                    | Possible Cause        | Solution                                               |
| -------------------------- | --------------------- | ------------------------------------------------------ |
| High loss doesn't decrease | Learning rate issues  | Decrease `rlsDelta`, increase `reservoirSize`          |
| Predictions are constant   | Dead reservoir        | Increase `inputScale`, check `spectralRadius < 1`      |
| Predictions explode        | Numerical instability | Decrease `spectralRadius`, increase `l2Lambda`         |
| Slow training              | Large reservoir       | Decrease `reservoirSize`, increase `reservoirSparsity` |
| Poor long-term predictions | Short memory          | Increase `spectralRadius`, decrease `leakRate`         |
| Can't track fast changes   | Slow adaptation       | Decrease `rlsLambda`, increase `leakRate`              |

---

## ğŸ“„ License

MIT License Â© 2025 [Henrique Emanoel Viana](https://github.com/hviana)

---

<div align="center">

**[â¬† Back to Top](#-esnregression)**

Made with â¤ï¸ for time series prediction

</div>
